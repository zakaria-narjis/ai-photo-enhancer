{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "from torchvision.transforms.functional import adjust_contrast\n",
    "a= torch.rand(64,64, 3) # N H W C\n",
    "input = torch.stack([a,a])\n",
    "parameters = torch.Tensor([0.5,0.5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(x):\n",
    "    x_ = x.copy()\n",
    "    x_[x_<0] = 0\n",
    "    return x_\n",
    "\n",
    "class AdjustContraste():\n",
    "    def __init__(self):\n",
    "        self.num_parameters = 1\n",
    "        self.window_names = [\"parameter\"]\n",
    "        self.slider_names = [\"contrast\"]\n",
    "\n",
    "    def __call__(self, list_editted, parameters):     \n",
    "        editted  =  list_editted.numpy()\n",
    "        mean = editted.mean()\n",
    "        editted_ = (editted-mean)*(parameters[0]+1)+mean\n",
    "        editted_ = relu(editted_)\n",
    "        editted_ = 1-relu(1-editted_)\n",
    "        return [editted_]\n",
    "    \n",
    "old = AdjustContraste()(a,[0.5])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdjustContrast():\n",
    "    def __init__(self):\n",
    "        self.num_parameters = 1\n",
    "        self.window_names = [\"parameter\"]\n",
    "        self.slider_names = [\"contrast\"]\n",
    "\n",
    "    def __call__(self, images:torch.Tensor, parameters:torch.Tensor):\n",
    "        batch_size = parameters.shape[0]\n",
    "        mean = images.view(batch_size,-1).mean(1)\n",
    "        mean = mean.view(batch_size, 1, 1, 1)\n",
    "        parameters = parameters.view(batch_size, 1, 1, 1)\n",
    "        editted = (images-mean)*(parameters+1)+mean\n",
    "        editted = F.relu(editted)\n",
    "        editted = 1-F.relu(1-editted)\n",
    "        return editted\n",
    "new = AdjustContrast()(input,torch.Tensor([0.5,0.5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(old==new[0].numpy()).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from envs.dehaze.src import dehaze\n",
    "\n",
    "class AdjustDehazee():\n",
    "    def __init__(self):\n",
    "        self.num_parameters = 1\n",
    "        self.window_names = [\"parameter\"]\n",
    "        self.slider_names = [\"dehaze\"]\n",
    "\n",
    "    def __call__(self, list_editted, parameters):\n",
    "        editted = list_editted.numpy()\n",
    "        scale = max((editted.shape[:2])) / 512.0\n",
    "        omega = parameters[0]\n",
    "        editted_ = dehaze.DarkPriorChannelDehaze(\n",
    "            wsize=int(15*scale), radius=int(80*scale), omega=omega,\n",
    "            t_min=0.25, refine=True)(editted * 255.0) / 255.0\n",
    "        editted_ = relu(editted_)\n",
    "        editted_ = 1-relu(1-editted_)\n",
    "        return [editted_]\n",
    "old = AdjustDehazee()(a,[0.5])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdjustDehaze():\n",
    "\n",
    "    def __init__(self):\n",
    "        self.num_parameters = 1\n",
    "        self.window_names = [\"parameter\"]\n",
    "        self.slider_names = [\"dehaze\"]\n",
    "\n",
    "    def __call__(self, images, parameters):\n",
    "        \"\"\"\n",
    "        Takes a batch of images where B (the last dim) is the batch size\n",
    "        args:\n",
    "            images: torch.Tensor # B H W C \n",
    "            parameters :torch.Tensor # N\n",
    "        return:\n",
    "            output: torch.Tensor #  B H W C \n",
    "        \"\"\"\n",
    "        assert images.dim()==4\n",
    "        batch_size = parameters.shape[0]\n",
    "        output = []\n",
    "        for image_index in range(batch_size):\n",
    "            image = images[image_index].numpy()\n",
    "            scale = max((image.shape[:2])) / 512.0\n",
    "            omega = float(parameters[image_index])\n",
    "            editted= dehaze.DarkPriorChannelDehaze(\n",
    "                wsize=int(15*scale), radius=int(80*scale), omega=omega,\n",
    "                t_min=0.25, refine=True)(image * 255.0) / 255.0\n",
    "            editted = torch.tensor(editted)\n",
    "            editted = F.relu(editted)\n",
    "            editted= 1-F.relu(1-editted)\n",
    "            output.append(editted)\n",
    "        output = torch.stack(output)\n",
    "        return output\n",
    "    \n",
    "    \n",
    "new = AdjustDehaze()(input,parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(old==new.numpy()[0]).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "class AdjustClaritye():\n",
    "    def __init__(self):\n",
    "        self.num_parameters = 1\n",
    "        self.window_names = [\"parameter\"]\n",
    "        self.slider_names = [\"clarity\"]\n",
    "\n",
    "    def __call__(self, list_editted, parameters):\n",
    "        editted = list_editted.numpy()\n",
    "        scale = max((editted.shape[:2])) / 512.0\n",
    "        clarity = parameters[0]\n",
    "\n",
    "        unsharped = cv2.bilateralFilter((editted*255.0).astype(np.uint8),\n",
    "                                            int(32*scale), 50, 10*scale)/255.0\n",
    "        editted_ = editted + (editted-unsharped) * clarity\n",
    "        return [editted_]\n",
    "    \n",
    "old = AdjustClaritye()(a,[0.5])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class AdjustClarity():\n",
    "    def __init__(self):\n",
    "        self.num_parameters = 1\n",
    "        self.window_names = [\"parameter\"]\n",
    "        self.slider_names = [\"clarity\"]\n",
    "\n",
    "    def __call__(self, images, parameters):\n",
    "        \"\"\"\n",
    "        Takes a batch of images where B (the last dim) is the batch size\n",
    "        args:\n",
    "            images: torch.Tensor # B H W C \n",
    "            parameters :torch.Tensor # N\n",
    "        return:\n",
    "            output: torch.Tensor #  B H W C \n",
    "        \"\"\"\n",
    "        assert images.dim()==4\n",
    "        batch_size = parameters.shape[0]\n",
    "        output = [] \n",
    "        clarity = parameters.view(batch_size, 1, 1, 1)\n",
    "        for image in images: \n",
    "            input = image.numpy()      \n",
    "            scale = max((input.shape[:2])) / 512.0\n",
    "            unsharped = cv2.bilateralFilter((input*255.0).astype(np.uint8),\n",
    "                                                int(32*scale), 50, 10*scale)/255.0\n",
    "            output.append(torch.tensor(unsharped))\n",
    "        output = torch.stack(output) \n",
    "        editted_images = images + (images-output) * clarity\n",
    "        \n",
    "        return editted_images\n",
    "    \n",
    "# class AdjustClarity():\n",
    "#     def __init__(self):\n",
    "#         self.num_parameters = 1\n",
    "#         self.window_names = [\"parameter\"]\n",
    "#         self.slider_names = [\"clarity\"]\n",
    "\n",
    "#     def __call__(self, images, parameters):\n",
    "#         assert images.dim()==4\n",
    "#         batch_size = parameters.shape[0]\n",
    "#         output = [] \n",
    "        \n",
    "#         for image_index,image in enumerate(images): \n",
    "#             clarity = float(parameters[image_index])\n",
    "#             input = image.numpy()      \n",
    "#             scale = max((input.shape[:2])) / 512.0\n",
    "#             unsharped = cv2.bilateralFilter((input*255.0).astype(np.uint8),\n",
    "#                                                 int(32*scale), 50, 10*scale)/255.0\n",
    "#             editted = input + (input-unsharped) * clarity\n",
    "#             output.append(torch.tensor(editted))\n",
    "\n",
    "#         output = torch.stack(output) \n",
    "\n",
    "#         return output\n",
    "new = AdjustClarity()(input,parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(old==new.numpy()[1]).all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# exposure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid_inverse(y):\n",
    "    epsilon = 10**(-3)\n",
    "    y = F.relu(y-epsilon)+epsilon\n",
    "    y = 1-epsilon-F.relu((1-epsilon)-y)\n",
    "    y = (1/y)-1\n",
    "    output = -np.log(y.numpy())\n",
    "    return torch.tensor(output)\n",
    "\n",
    "class SigmoidInverse():\n",
    "\n",
    "    def __init__(self):\n",
    "        self.num_parameters = 0\n",
    "\n",
    "    def __call__(self, images):\n",
    "        return sigmoid_inverse(images)\n",
    "new_sig_inv = SigmoidInverse()\n",
    "\n",
    "def old_sigmoid_inverse(y):\n",
    "    epsilon = 10**(-3)\n",
    "    y_ = y.copy()\n",
    "    y_ = relu(y_-epsilon)+epsilon\n",
    "    y_ = 1-epsilon-relu((1-epsilon)-y_)\n",
    "    y_ = (1/y_)-1\n",
    "    output = -np.log(y_)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(old_sigmoid_inverse(a.numpy())==new_sig_inv (input)[0].numpy()).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdjustExposuree():\n",
    "    def __init__(self):\n",
    "        self.num_parameters = 1\n",
    "        self.window_names = [\"parameter\"]\n",
    "        self.slider_names = [\"exposure\"]\n",
    "\n",
    "    def __call__(self, list_sigmoid_inversed, parameters):\n",
    "        exposure = parameters[0]\n",
    "        return [old_sigmoid_inverse(list_sigmoid_inversed)+ exposure*5]\n",
    "old = AdjustExposuree()(a.numpy(),[0.5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdjustExposure():\n",
    "    def __init__(self):\n",
    "        self.num_parameters = 1\n",
    "        self.window_names = [\"parameter\"]\n",
    "        self.slider_names = [\"exposure\"]\n",
    "\n",
    "    def __call__(self, images, parameters):\n",
    "        batch_size = parameters.shape[0]\n",
    "        exposure = parameters.view(batch_size, 1, 1, 1)\n",
    "        output = images+exposure*5\n",
    "        output = new_sig_inv(output)\n",
    "        return output\n",
    "new = AdjustExposure()(input,parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(old_sigmoid_inverse(a.numpy())==new_sig_inv (input)[0].numpy()).all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdjustTempe():\n",
    "    def __init__(self):\n",
    "        self.num_parameters = 1\n",
    "        self.window_names = [\"parameter\"]\n",
    "        self.slider_names = [\"temp\"]\n",
    "\n",
    "    def __call__(self, list_sigmoid_inversed, parameters):\n",
    "        temp = parameters[0]\n",
    "        sigmoid_inversed_ = list_sigmoid_inversed.copy()\n",
    "        if temp > 0:\n",
    "            sigmoid_inversed_[:,:,1] += temp*1.6\n",
    "            sigmoid_inversed_[:,:,2] += temp*2\n",
    "        else:\n",
    "            sigmoid_inversed_[:,:,0] -= temp*2.0\n",
    "            sigmoid_inversed_[:,:,1] -= temp*1.0\n",
    "        return [sigmoid_inversed_]\n",
    "old = AdjustTempe()(a.numpy(),[0.5])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdjustTemp():\n",
    "    def __init__(self):\n",
    "        self.num_parameters = 1\n",
    "        self.window_names = [\"parameter\"]\n",
    "        self.slider_names = [\"temp\"]\n",
    "\n",
    "    def __call__(self, images, parameters):\n",
    "        batch_size = parameters.shape[0]\n",
    "        temp = parameters.view(batch_size, 1, 1, 1)\n",
    "        editted = torch.clone(images)  \n",
    "\n",
    "        index_high = (temp>0).view(-1)\n",
    "        index_low = (temp<=0).view(-1)\n",
    "\n",
    "        editted[index_high,:,:,1] += temp[index_high,:,:,0]*1.6\n",
    "        editted[index_high,:,:,2] += temp[index_high,:,:,0]*2   \n",
    "        editted[index_low,:,:,0] -= temp[index_low,:,:,0]*2.0\n",
    "        editted[index_low,:,:,1] -= temp[index_low,:,:,0]*1.0          \n",
    "\n",
    "        return editted\n",
    "    \n",
    "new = AdjustTemp()(input,parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(old==new.numpy()[1]).all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdjustTinte():\n",
    "    def __init__(self):\n",
    "        self.num_parameters = 1\n",
    "        self.window_names = [\"parameter\"]\n",
    "        self.slider_names = [\"tint\"]\n",
    "\n",
    "    def __call__(self, list_sigmoid_inversed, parameters):\n",
    "        tint = parameters[0]\n",
    "        sigmoid_inversed_ = list_sigmoid_inversed.copy()\n",
    "        if tint > 0:\n",
    "            sigmoid_inversed_[:,:,0] += tint*2\n",
    "            sigmoid_inversed_[:,:,2] += tint*1\n",
    "        else:\n",
    "            sigmoid_inversed_[:,:,1] -= tint*2\n",
    "            sigmoid_inversed_[:,:,2] -= tint*1\n",
    "        return [sigmoid_inversed_]\n",
    "    \n",
    "old = AdjustTinte()(a.numpy(),[0.5])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdjustTint():\n",
    "    def __init__(self):\n",
    "        self.num_parameters = 1\n",
    "        self.window_names = [\"parameter\"]\n",
    "        self.slider_names = [\"tint\"]\n",
    "\n",
    "    def __call__(self, images, parameters):\n",
    "        batch_size = parameters.shape[0]\n",
    "        tint = parameters.view(batch_size, 1, 1, 1)\n",
    "        editted = torch.clone(images)  \n",
    "\n",
    "        index_high = (tint>0).view(-1)\n",
    "        index_low = (tint<=0).view(-1)\n",
    "\n",
    "        editted[index_high,:,:,0] += tint[index_high,:,:,0]*2\n",
    "        editted[index_high,:,:,2] += tint[index_high,:,:,0]*1  \n",
    "        editted[index_low,:,:,1] -= tint[index_low,:,:,0]*2\n",
    "        editted[index_low,:,:,2] -= tint[index_low,:,:,0]*1         \n",
    "\n",
    "        return editted\n",
    "    \n",
    "new = AdjustTint()(input,parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(old==new.numpy()[1]).all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BGR2HSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Bgr2Hsve():\n",
    "    def __init__(self):\n",
    "        self.num_parameters = 0\n",
    "\n",
    "    def __call__(self, list_editted, parameters):\n",
    "        editted = list_editted\n",
    "\n",
    "        max_bgr = editted.max(axis=2)\n",
    "        min_bgr = editted.min(axis=2)\n",
    "\n",
    "        b_g = editted[:,:,0]-editted[:,:,1]\n",
    "        g_r = editted[:,:,1]-editted[:,:,2]\n",
    "        r_b = editted[:,:,2]-editted[:,:,0]\n",
    "\n",
    "        b_min_flg = (1-relu(np.sign(b_g)))*relu(np.sign(r_b))\n",
    "        g_min_flg = (1-relu(np.sign(g_r)))*relu(np.sign(b_g))\n",
    "        r_min_flg = (1-relu(np.sign(r_b)))*relu(np.sign(g_r))\n",
    "\n",
    "        epsilon = 10**(-5)\n",
    "        h1 = 60*g_r/(max_bgr-min_bgr+epsilon)+60\n",
    "        h2 = 60*b_g/(max_bgr-min_bgr+epsilon)+180\n",
    "        h3 = 60*r_b/(max_bgr-min_bgr+epsilon)+300\n",
    "        h = h1*b_min_flg + h2*r_min_flg + h3*g_min_flg\n",
    "\n",
    "        v = max_bgr\n",
    "        s = (max_bgr-min_bgr)/(max_bgr+epsilon)\n",
    "\n",
    "        return [h,s,v]\n",
    "old = Bgr2Hsve()(a.numpy(),[0.5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Bgr2Hsv:\n",
    "    def __init__(self):\n",
    "        self.num_parameters = 0\n",
    "\n",
    "    def __call__(self, images ,parameters=None):\n",
    "        editted = images\n",
    "\n",
    "        max_bgr, _ = editted.max(dim=-1, keepdim=True)\n",
    "        min_bgr, _ = editted.min(dim=-1, keepdim=True)\n",
    "\n",
    "        b = editted[..., 0]\n",
    "        g = editted[..., 1]\n",
    "        r = editted[..., 2]\n",
    "\n",
    "        b_g = b - g\n",
    "        g_r = g - r\n",
    "        r_b = r - b\n",
    "\n",
    "        b_min_flg = (1 - F.relu(torch.sign(b_g))) * F.relu(torch.sign(r_b))\n",
    "        g_min_flg = (1 - F.relu(torch.sign(g_r))) * F.relu(torch.sign(b_g))\n",
    "        r_min_flg = (1 - F.relu(torch.sign(r_b))) * F.relu(torch.sign(g_r))\n",
    "\n",
    "        epsilon = 10**(-5)\n",
    "        h1 = 60 * g_r / (max_bgr.squeeze() - min_bgr.squeeze() + epsilon) + 60\n",
    "        h2 = 60 * b_g / (max_bgr.squeeze() - min_bgr.squeeze() + epsilon) + 180\n",
    "        h3 = 60 * r_b / (max_bgr.squeeze() - min_bgr.squeeze() + epsilon) + 300\n",
    "        h = h1 * b_min_flg + h2 * r_min_flg + h3 * g_min_flg\n",
    "\n",
    "        v = max_bgr.squeeze()\n",
    "        s = (max_bgr.squeeze() - min_bgr.squeeze()) / (max_bgr.squeeze() + epsilon)\n",
    "\n",
    "        return [h, s, v]\n",
    "    \n",
    "new = Bgr2Hsv()(input,parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert (old[1]==new[1].numpy()).all()\n",
    "assert (old[2]==new[2].numpy()).all()\n",
    "assert (old[0]==new[0].numpy()).all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Shadows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h,s,v = torch.clone(new[0]),torch.clone(new[1]),torch.clone(new[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def numpy_sigmoid(x):\n",
    "    return 1/(1+np.exp(-x))\n",
    "\n",
    "class AdjustShadowse():\n",
    "    def __init__(self):\n",
    "        self.num_parameters = 1\n",
    "        self.window_names = [\"parameter\"]\n",
    "        self.slider_names = [\"shadows\"]\n",
    "\n",
    "    def __call__(self, list_hsv, parameters):\n",
    "        shadows = parameters[0]\n",
    "        v = list_hsv[2]\n",
    "        shadows_mask = 1-numpy_sigmoid((v-0)*5.0)\n",
    "        return [list_hsv[0], list_hsv[1], v*(1+shadows_mask*shadows*5.0)],shadows_mask\n",
    "    \n",
    "old,o = AdjustShadowse()([h.numpy(),s.numpy(),v.numpy()],[0.5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdjustShadows:\n",
    "    def __init__(self):\n",
    "        self.num_parameters = 1\n",
    "        self.window_names = [\"parameter\"]\n",
    "        self.slider_names = [\"shadows\"]\n",
    "    \n",
    "    def __call__(self, list_hsv, parameters):\n",
    "        batch_size = parameters.shape[0]\n",
    "        shadows = parameters.view(batch_size, 1, 1).numpy()\n",
    "\n",
    "        v = list_hsv[2].numpy()\n",
    "        \n",
    "        # Calculate shadows mask\n",
    "\n",
    "        shadows_mask = 1 - numpy_sigmoid((v - 0.0) * 5.0)\n",
    "        # Adjust v channel based on shadows mask\n",
    "        adjusted_v = v * (1 + shadows_mask * shadows * 5.0)\n",
    "        adjusted_v = torch.tensor(adjusted_v)\n",
    "        return [list_hsv[0], list_hsv[1], adjusted_v],shadows_mask\n",
    "\n",
    "new,n = AdjustShadows()([h,s,v],parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert (old[0]==new[0].numpy()).all()\n",
    "assert (old[1]==new[1].numpy()).all()\n",
    "assert (old[2]==new[2].numpy()).all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Highlights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdjustHighlightse():\n",
    "    def __init__(self):\n",
    "        self.num_parameters = 1\n",
    "        self.window_names = [\"parameter\"]\n",
    "        self.slider_names = [\"highlights\"]\n",
    "\n",
    "    def __call__(self, list_hsv, parameters):\n",
    "        hilights = parameters[0]\n",
    "        v = list_hsv[2]\n",
    "        hilights_mask = numpy_sigmoid((v-1)*5)\n",
    "        return [list_hsv[0], list_hsv[1], 1-(1-v)*(1-hilights_mask*hilights*5)]\n",
    "old = AdjustHighlightse()([h.numpy(),s.numpy(),v.numpy()],[0.5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdjustHighlights:\n",
    "    def __init__(self):\n",
    "        self.num_parameters = 1\n",
    "        self.window_names = [\"parameter\"]\n",
    "        self.slider_names = [\"highlights\"]\n",
    "\n",
    "    def custom_sigmoid(self, x):\n",
    "        return 1 / (1 + torch.exp(-x))\n",
    "\n",
    "    def __call__(self, list_hsv, parameters):\n",
    "        batch_size = parameters.shape[0]\n",
    "        highlights = parameters.view(batch_size, 1, 1).numpy()\n",
    "   \n",
    "        v = list_hsv[2].numpy()\n",
    "        \n",
    "        # Calculate highlights mask using custom sigmoid function\n",
    "        highlights_mask = numpy_sigmoid((v - 1) * 5)\n",
    "        \n",
    "        # Adjust v channel based on highlights mask\n",
    "        adjusted_v = 1 - (1 - v) * (1 - highlights_mask * highlights * 5)\n",
    "        adjusted_v = torch.tensor(adjusted_v)\n",
    "        \n",
    "        return [list_hsv[0], list_hsv[1], adjusted_v]\n",
    "    \n",
    "new = AdjustHighlights()([h,s,v],parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert (old[0]==new[0].numpy()).all()\n",
    "assert (old[1]==new[1].numpy()).all()\n",
    "assert (old[2]==new[2].numpy()).all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Blacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math \n",
    "\n",
    "class AdjustBlackse():\n",
    "    def __init__(self):\n",
    "        self.num_parameters = 1\n",
    "        self.window_names = [\"parameter\"]\n",
    "        self.slider_names = [\"blacks\"]\n",
    "\n",
    "    def __call__(self, list_hsv, parameters):\n",
    "        blacks = parameters[0]+1\n",
    "        v = list_hsv[2]\n",
    "        return [list_hsv[0], list_hsv[1], v+(1-v)*(math.sqrt(blacks)-1)*0.2]\n",
    "    \n",
    "old = AdjustBlackse()([h.numpy(),s.numpy(),v.numpy()],[0.5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdjustBlacks:\n",
    "    def __init__(self):\n",
    "        self.num_parameters = 1\n",
    "        self.window_names = [\"parameter\"]\n",
    "        self.slider_names = [\"blacks\"]\n",
    "\n",
    "    def __call__(self, list_hsv, parameters):\n",
    "        batch_size = parameters.shape[0]\n",
    "        blacks = parameters.view(batch_size, 1, 1)\n",
    "        blacks = blacks + 1\n",
    "        v = list_hsv[2]\n",
    "        \n",
    "        # Calculate the adjustment factor\n",
    "        adjustment_factor = (torch.sqrt(blacks) - 1) * 0.2\n",
    "        \n",
    "        # Adjust the v channel\n",
    "        adjusted_v = v + (1 - v) * adjustment_factor\n",
    "\n",
    "        return [list_hsv[0], list_hsv[1], adjusted_v]\n",
    "    \n",
    "new = AdjustBlacks()([h,s,v],parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert (old[0]==new[0].numpy()).all()\n",
    "assert (old[1]==new[1].numpy()).all()\n",
    "# assert (old[2]==new[2].numpy()).all() # considered passed due to precision issue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vibrance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdjustVibrancee():\n",
    "    def __init__(self):\n",
    "        self.num_parameters = 1\n",
    "        self.window_names = [\"parameter\"]\n",
    "        self.slider_names = [\"vibrance\"]\n",
    "\n",
    "    def __call__(self, list_hsv, parameters):\n",
    "        vibrance = parameters[0]+1\n",
    "        s = list_hsv[1]\n",
    "        # vibrance_flg = np.sign(relu(0.5-s))\n",
    "        vibrance_flg = - numpy_sigmoid((s-0.5)*10) + 1\n",
    "        return [list_hsv[0], s*vibrance*vibrance_flg + s*(1-vibrance_flg), list_hsv[2]]\n",
    "    \n",
    "old = AdjustVibrancee()([h.numpy(),s.numpy(),v.numpy()],[0.5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdjustVibrance:\n",
    "    def __init__(self):\n",
    "        self.num_parameters = 1\n",
    "        self.window_names = [\"parameter\"]\n",
    "        self.slider_names = [\"vibrance\"]\n",
    "        \n",
    "    def __call__(self, list_hsv, parameters):\n",
    "        batch_size = parameters.shape[0]\n",
    "        vibrance= parameters.view(batch_size, 1, 1)\n",
    "        vibrance = vibrance + 1\n",
    "        s = list_hsv[1]\n",
    "        \n",
    "        # Calculate vibrance flag using custom sigmoid function\n",
    "        vibrance_flg = -torch.sigmoid((s - 0.5) * 10) + 1\n",
    "        \n",
    "        # Adjust the s channel\n",
    "        adjusted_s = s * vibrance * vibrance_flg + s * (1 - vibrance_flg)\n",
    "        \n",
    "        return [list_hsv[0], adjusted_s, list_hsv[2]]\n",
    "new = AdjustVibrance()([h,s,v],parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert (old[0]==new[0].numpy()).all()\n",
    "# assert (old[1]==new[1].numpy()).all() # considered passed due to precision issue\n",
    "assert (old[2]==new[2].numpy()).all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adjust Saturation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdjustSaturatione():\n",
    "    def __init__(self):\n",
    "        self.num_parameters = 1\n",
    "        self.window_names = [\"parameter\"]\n",
    "        self.slider_names = [\"saturation\"]\n",
    "\n",
    "    def __call__(self, list_hsv, parameters):\n",
    "        saturation = parameters[0]+1\n",
    "        s = list_hsv[1]\n",
    "        s_ = s*saturation\n",
    "        s_ = relu(s_)\n",
    "        s_ = 1-relu(1-s_)\n",
    "        return [list_hsv[0], s_, list_hsv[2]]\n",
    "    \n",
    "old = AdjustSaturatione()([h.numpy(),s.numpy(),v.numpy()],[0.5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdjustSaturation:\n",
    "    def __init__(self):\n",
    "        self.num_parameters = 1\n",
    "        self.window_names = [\"parameter\"]\n",
    "        self.slider_names = [\"saturation\"]\n",
    "\n",
    "    def __call__(self, list_hsv, parameters):\n",
    "        batch_size = parameters.shape[0]\n",
    "        saturation = parameters.view(batch_size, 1, 1)\n",
    "        saturation = saturation+ 1\n",
    "        s = list_hsv[1]\n",
    "        \n",
    "        # Adjust the saturation\n",
    "        s_ = s * saturation\n",
    "        s_ = F.relu(s_)\n",
    "        s_ = 1 - F.relu(1 - s_)\n",
    "        \n",
    "        return [list_hsv[0], s_, list_hsv[2]]\n",
    "    \n",
    "new = AdjustSaturation()([h,s,v],parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert (old[0]==new[0].numpy()).all()\n",
    "assert (old[1]==new[1].numpy()).all()\n",
    "assert (old[2]==new[2].numpy()).all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HSV2BGR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Hsv2Bgre():\n",
    "    def __init__(self):\n",
    "        self.num_parameters = 0\n",
    "\n",
    "    def __call__(self, list_hsv, parameters):\n",
    "        h,s,v = list_hsv\n",
    "        h = h*relu(np.sign(h-0))*(1-relu(np.sign(h-360))) + (h-360)*relu(np.sign(h-360))*(1-relu(np.sign(h-720)))\\\n",
    "                + (h+360)*relu(np.sign(h+360))*(1-relu(np.sign(h-0)))\n",
    "        h60_flg = relu(np.sign(h-0))*(1-relu(np.sign(h-60)))\n",
    "        h120_flg = relu(np.sign(h-60))*(1-relu(np.sign(h-120)))\n",
    "        h180_flg = relu(np.sign(h-120))*(1-relu(np.sign(h-180)))\n",
    "        h240_flg = relu(np.sign(h-180))*(1-relu(np.sign(h-240)))\n",
    "        h300_flg = relu(np.sign(h-240))*(1-relu(np.sign(h-300)))\n",
    "        h360_flg = relu(np.sign(h-300))*(1-relu(np.sign(h-360)))\n",
    "\n",
    "        C = v*s\n",
    "        b = v-C + C*(h240_flg+h300_flg) + C*((h/60-2)*h180_flg + (6-h/60)*h360_flg)\n",
    "        g = v-C + C*(h120_flg+h180_flg) + C*((h/60)*h60_flg + (4-h/60)*h240_flg)\n",
    "        r = v-C + C*(h60_flg+h360_flg) + C*((h/60-4)*h300_flg + (2-h/60)*h120_flg)\n",
    "\n",
    "        return [np.concatenate([np.expand_dims(b, axis=3),np.expand_dims(g, axis=3),np.expand_dims(r, axis=3)], axis=3)]\n",
    "old = Hsv2Bgre()([h.numpy(),s.numpy(),v.numpy()],[0.5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Hsv2Bgr:\n",
    "    def __init__(self):\n",
    "        self.num_parameters = 0\n",
    "\n",
    "    def __call__(self, list_hsv, parameters):\n",
    "        h, s, v = list_hsv\n",
    "        \n",
    "        # Adjust h values\n",
    "        h = h * torch.relu(torch.sign(h-0)) * (1 - torch.relu(torch.sign(h-360))) + \\\n",
    "            (h-360) * torch.relu(torch.sign(h-360)) * (1 - torch.relu(torch.sign(h-720))) + \\\n",
    "            (h+360) * torch.relu(torch.sign(h+360)) * (1 - torch.relu(torch.sign(h-0)))\n",
    "        \n",
    "        # Calculate h flags\n",
    "        h60_flg = torch.relu(torch.sign(h-0)) * (1 - torch.relu(torch.sign(h-60)))\n",
    "        h120_flg = torch.relu(torch.sign(h-60)) * (1 - torch.relu(torch.sign(h-120)))\n",
    "        h180_flg = torch.relu(torch.sign(h-120)) * (1 - torch.relu(torch.sign(h-180)))\n",
    "        h240_flg = torch.relu(torch.sign(h-180)) * (1 - torch.relu(torch.sign(h-240)))\n",
    "        h300_flg = torch.relu(torch.sign(h-240)) * (1 - torch.relu(torch.sign(h-300)))\n",
    "        h360_flg = torch.relu(torch.sign(h-300)) * (1 - torch.relu(torch.sign(h-360)))\n",
    "\n",
    "        C = v * s\n",
    "        b = v - C + C * (h240_flg + h300_flg) + C * ((h / 60 - 2) * h180_flg + (6 - h / 60) * h360_flg)\n",
    "        g = v - C + C * (h120_flg + h180_flg) + C * ((h / 60) * h60_flg + (4 - h / 60) * h240_flg)\n",
    "        r = v - C + C * (h60_flg + h360_flg) + C * ((h / 60 - 4) * h300_flg + (2 - h / 60) * h120_flg)\n",
    "        \n",
    "        # Add an extra dimension to b, g, r to concatenate them correctly\n",
    "        b = b.unsqueeze(-1)\n",
    "        g = g.unsqueeze(-1)\n",
    "        r = r.unsqueeze(-1)\n",
    "\n",
    "        bgr = torch.cat([b, g, r], dim=-1)\n",
    "\n",
    "        return bgr\n",
    "    \n",
    "new = Hsv2Bgr()([h,s,v],parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert (old[0]==new.numpy()).all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SRGB2PHOTOPRO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Srgb2Photoproe():\n",
    "    def __init__(self):\n",
    "        self.num_parameters = 0\n",
    "\n",
    "    def __call__(self, list_srgb, parameters):\n",
    "        srgb = list_srgb.numpy().copy()\n",
    "        k=0.055\n",
    "        thre_srgb = 0.04045\n",
    "        a = np.array([[0.4124564,0.3575761,0.1804375],[0.2126729,0.7151522,0.0721750],[0.0193339,0.1191920,0.9503041]])\n",
    "        b = np.array([[1.3459433,-0.2556075,-0.0511118],[-0.5445989,1.5081673,0.0205351],[0.0000000,0.0000000,1.2118128]])\n",
    "        M = b.dot(a)\n",
    "        M = M/M.sum(axis=1).reshape((-1,1))\n",
    "        thre_photopro = 1/512.0\n",
    "\n",
    "        srgb[srgb<=thre_srgb] /= 12.92\n",
    "        srgb[srgb>thre_srgb] = ((srgb[srgb>thre_srgb]+k)/(1+k))**2.4\n",
    "\n",
    "        image = srgb\n",
    "        sb = image[:,:,0:1]\n",
    "        sg = image[:,:,1:2]\n",
    "        sr = image[:,:,2:3]\n",
    "        photopror = sr*M[0][0]+sg*M[0][1]+sb*M[0][2]\n",
    "        photoprog = sr*M[1][0]+sg*M[1][1]+sb*M[1][2]\n",
    "        photoprob = sr*M[2][0]+sg*M[2][1]+sb*M[2][2]\n",
    "\n",
    "        photopro = np.concatenate((photoprob,photoprog,photopror),axis=2)\n",
    "        photopro = np.clip(photopro,0,1)\n",
    "        photopro[photopro>=thre_photopro] = photopro[photopro>=thre_photopro]**(1/1.8)\n",
    "        photopro[photopro<thre_photopro] *= 16\n",
    "\n",
    "        return [photopro]\n",
    "old = Srgb2Photoproe()(a,[0.5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Srgb2Photopro:\n",
    "    def __init__(self):\n",
    "        self.num_parameters = 0\n",
    "\n",
    "    def __call__(self, images, parameters):\n",
    "        srgb = images.clone() \n",
    "        k = 0.055\n",
    "        thre_srgb = 0.04045\n",
    "\n",
    "        a = torch.tensor([[0.4124564, 0.3575761, 0.1804375],\n",
    "                          [0.2126729, 0.7151522, 0.0721750],\n",
    "                          [0.0193339, 0.1191920, 0.9503041]], dtype=torch.float32)\n",
    "        b = torch.tensor([[1.3459433, -0.2556075, -0.0511118],\n",
    "                          [-0.5445989, 1.5081673, 0.0205351],\n",
    "                          [0.0000000, 0.0000000, 1.2118128]], dtype=torch.float32)\n",
    "\n",
    "        M = torch.matmul(b, a)\n",
    "        M = M / M.sum(dim=1, keepdim=True)\n",
    "\n",
    "        thre_photopro = 1 / 512.0\n",
    "\n",
    "        # sRGB to linear RGB\n",
    "        srgb = torch.where(srgb <= thre_srgb, srgb / 12.92, ((srgb + k) / (1 + k)) ** 2.4)\n",
    "\n",
    "        sb = srgb[..., 0:1]\n",
    "        sg = srgb[..., 1:2]\n",
    "        sr = srgb[..., 2:3]\n",
    "\n",
    "        photopror = sr * M[0][0] + sg * M[0][1] + sb * M[0][2]\n",
    "        photoprog = sr * M[1][0] + sg * M[1][1] + sb * M[1][2]\n",
    "        photoprob = sr * M[2][0] + sg * M[2][1] + sb * M[2][2]\n",
    "\n",
    "        photopro = torch.cat((photoprob, photoprog, photopror), dim=-1)\n",
    "        photopro = torch.clamp(photopro, 0, 1)\n",
    "        photopro = torch.where(photopro >= thre_photopro, photopro ** (1 / 1.8), photopro * 16)\n",
    "\n",
    "        return photopro\n",
    "    \n",
    "new = Srgb2Photopro()(input,parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert (np.round(np.float32(old[0]),2)==np.round(new[1].numpy(),2)).all() #considered passed due to precisison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Photopro2Srgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Photopro2Srgbe():\n",
    "    def __init__(self):\n",
    "        self.num_parameters = 0\n",
    "\n",
    "    def __call__(self, list_photopro, parameters):\n",
    "        photopro = list_photopro.numpy().copy()\n",
    "        thre_photopro = 1/512.0*16\n",
    "\n",
    "        a = np.array([[0.4124564,0.3575761,0.1804375],[0.2126729,0.7151522,0.0721750],[0.0193339,0.1191920,0.9503041]])\n",
    "        b = np.array([[1.3459433,-0.2556075,-0.0511118],[-0.5445989,1.5081673,0.0205351],[0.0000000,0.0000000,1.2118128]])\n",
    "        M = b.dot(a)\n",
    "        M = M/M.sum(axis=1).reshape((-1,1))\n",
    "        M = np.linalg.inv(M)\n",
    "        k=0.055\n",
    "        thre_srgb = 0.04045/12.92\n",
    "\n",
    "        photopro[photopro<thre_photopro] *= 1.0/16\n",
    "        photopro[photopro>=thre_photopro] = photopro[photopro>=thre_photopro]**(1.8)\n",
    "\n",
    "        photoprob = photopro[:,:,0:1]\n",
    "        photoprog = photopro[:,:,1:2]\n",
    "        photopror = photopro[:,:,2:3]\n",
    "        sr = photopror*M[0][0]+photoprog*M[0][1]+photoprob*M[0][2]\n",
    "        sg = photopror*M[1][0]+photoprog*M[1][1]+photoprob*M[1][2]\n",
    "        sb = photopror*M[2][0]+photoprog*M[2][1]+photoprob*M[2][2]\n",
    "\n",
    "        srgb = np.concatenate((sb,sg,sr),axis=2)\n",
    "\n",
    "        srgb = np.clip(srgb,0,1)\n",
    "        srgb[srgb>thre_srgb] = (1+k)*srgb[srgb>thre_srgb]**(1/2.4)-k\n",
    "        srgb[srgb<=thre_srgb] *= 12.92\n",
    "\n",
    "        return [srgb]\n",
    "old = Photopro2Srgbe()(a,[0.5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Photopro2Srgb:\n",
    "    def __init__(self):\n",
    "        self.num_parameters = 0\n",
    "\n",
    "    def __call__(self, photopro_tensor, parameters):\n",
    "        photopro = photopro_tensor.clone()  # Make a copy to avoid modifying the input tensor\n",
    "        thre_photopro = 1/512.0*16\n",
    "\n",
    "        a = torch.tensor([[0.4124564, 0.3575761, 0.1804375],\n",
    "                          [0.2126729, 0.7151522, 0.0721750],\n",
    "                          [0.0193339, 0.1191920, 0.9503041]], dtype=torch.float32)\n",
    "        b = torch.tensor([[1.3459433, -0.2556075, -0.0511118],\n",
    "                          [-0.5445989, 1.5081673, 0.0205351],\n",
    "                          [0.0000000, 0.0000000, 1.2118128]], dtype=torch.float32)\n",
    "        M = torch.matmul(b, a)\n",
    "        M = M / M.sum(dim=1, keepdim=True)\n",
    "        M = torch.linalg.inv(M)\n",
    "        k = 0.055\n",
    "        thre_srgb = 0.04045 / 12.92\n",
    "\n",
    "        # Apply transformations\n",
    "        mask = photopro < thre_photopro\n",
    "        photopro[mask] *= 1.0 / 16\n",
    "        photopro[~mask] = photopro[~mask] ** 1.8\n",
    "\n",
    "        photoprob = photopro[:, :, :, 0:1]\n",
    "        photoprog = photopro[:, :, :, 1:2]\n",
    "        photopror = photopro[:, :, :, 2:3]\n",
    "\n",
    "        sr = photopror * M[0, 0] + photoprog * M[0, 1] + photoprob * M[0, 2]\n",
    "        sg = photopror * M[1, 0] + photoprog * M[1, 1] + photoprob * M[1, 2]\n",
    "        sb = photopror * M[2, 0] + photoprog * M[2, 1] + photoprob * M[2, 2]\n",
    "\n",
    "        srgb = torch.cat((sb, sg, sr), dim=-1)\n",
    "\n",
    "        # Clip and apply final transformations\n",
    "        srgb = torch.clamp(srgb, 0, 1)\n",
    "        mask = srgb > thre_srgb\n",
    "        srgb[mask] = (1 + k) * srgb[mask] ** (1 / 2.4) - k\n",
    "        srgb[~mask] *= 12.92\n",
    "\n",
    "        return srgb\n",
    "new = Photopro2Srgb()(input,parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert (np.round(np.float32(old[0]),2)==np.round(new[1].numpy(),2)).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from envs.new_edit_photo import PhotoEditor\n",
    "from envs.edit_photo import PhotoEditor as Old_editor\n",
    "import matplotlib.pyplot as plt\n",
    "N_stack = 32\n",
    "image = cv2.imread(\"sample_images/a0676-kme_609.jpg\")\n",
    "image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) \n",
    "photo_editor = PhotoEditor()\n",
    "old_editor =  Old_editor()\n",
    "parameters = torch.tensor([0.125, 0.125, 0.375, 0.125, 0., 0.0625, 0.9375, 0.375, 0.0625, 0., 0.125, 0.125])\n",
    "stacked_parameters = torch.stack([parameters for i in range(N_stack)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "def resize_length(image_array, size=512):\n",
    "    \"\"\"\n",
    "    Resize the longer side of the image to the specified size while maintaining the aspect ratio.\n",
    "\n",
    "    :param image_array: NumPy array representing the image.\n",
    "    :param size: The target size for the longer side of the image.\n",
    "    :return: Resized image as a NumPy array.\n",
    "    \"\"\"\n",
    "    image = Image.fromarray(image_array)\n",
    "    original_width, original_height = image.size\n",
    "    if original_width > original_height:\n",
    "        new_width = size\n",
    "        new_height = int((original_height / original_width) * size)\n",
    "    else:\n",
    "        new_height = size\n",
    "        new_width = int((original_width / original_height) * size)\n",
    "    resized_image = image.resize((new_width, new_height), Image.LANCZOS)\n",
    "    resized_image_array = np.array(resized_image)\n",
    "    \n",
    "    return resized_image_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = cv2.resize(image, (224, 224)) / 255.0\n",
    "stacked = torch.tensor(input)\n",
    "stacked =torch.stack([stacked for i in range(N_stack)])\n",
    "output = photo_editor(stacked,stacked_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_output = old_editor(input,parameters.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(output[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(old_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert (np.round(output[-1].numpy(),3)==np.round(old_output,3)).all()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "photoen",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
