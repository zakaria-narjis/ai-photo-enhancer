{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 754,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "from torchvision.transforms.functional import adjust_contrast\n",
    "a= torch.rand(64,64, 3) # N H W C\n",
    "input = torch.stack([a,a])\n",
    "parameters = torch.Tensor([0.5,0.5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(x):\n",
    "    x_ = x.copy()\n",
    "    x_[x_<0] = 0\n",
    "    return x_\n",
    "\n",
    "class AdjustContraste():\n",
    "    def __init__(self):\n",
    "        self.num_parameters = 1\n",
    "        self.window_names = [\"parameter\"]\n",
    "        self.slider_names = [\"contrast\"]\n",
    "\n",
    "    def __call__(self, list_editted, parameters):     \n",
    "        editted  =  list_editted.numpy()\n",
    "        mean = editted.mean()\n",
    "        editted_ = (editted-mean)*(parameters[0]+1)+mean\n",
    "        editted_ = relu(editted_)\n",
    "        editted_ = 1-relu(1-editted_)\n",
    "        return [editted_]\n",
    "    \n",
    "old = AdjustContraste()(a,[0.5])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdjustContrast():\n",
    "    def __init__(self):\n",
    "        self.num_parameters = 1\n",
    "        self.window_names = [\"parameter\"]\n",
    "        self.slider_names = [\"contrast\"]\n",
    "\n",
    "    def __call__(self, images:torch.Tensor, parameters:torch.Tensor):\n",
    "        batch_size = parameters.shape[0]\n",
    "        mean = images.view(batch_size,-1).mean(1)\n",
    "        mean = mean.view(batch_size, 1, 1, 1)\n",
    "        parameters = parameters.view(batch_size, 1, 1, 1)\n",
    "        editted = (images-mean)*(parameters+1)+mean\n",
    "        editted = F.relu(editted)\n",
    "        editted = 1-F.relu(1-editted)\n",
    "        return editted\n",
    "new = AdjustContrast()(input,torch.Tensor([0.5,0.5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.True_"
      ]
     },
     "execution_count": 368,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(old==new[0].numpy()).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {},
   "outputs": [],
   "source": [
    "from envs.dehaze.src import dehaze\n",
    "\n",
    "class AdjustDehazee():\n",
    "    def __init__(self):\n",
    "        self.num_parameters = 1\n",
    "        self.window_names = [\"parameter\"]\n",
    "        self.slider_names = [\"dehaze\"]\n",
    "\n",
    "    def __call__(self, list_editted, parameters):\n",
    "        editted = list_editted.numpy()\n",
    "        scale = max((editted.shape[:2])) / 512.0\n",
    "        omega = parameters[0]\n",
    "        editted_ = dehaze.DarkPriorChannelDehaze(\n",
    "            wsize=int(15*scale), radius=int(80*scale), omega=omega,\n",
    "            t_min=0.25, refine=True)(editted * 255.0) / 255.0\n",
    "        editted_ = relu(editted_)\n",
    "        editted_ = 1-relu(1-editted_)\n",
    "        return [editted_]\n",
    "old = AdjustDehazee()(a,[0.5])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdjustDehaze():\n",
    "\n",
    "    def __init__(self):\n",
    "        self.num_parameters = 1\n",
    "        self.window_names = [\"parameter\"]\n",
    "        self.slider_names = [\"dehaze\"]\n",
    "\n",
    "    def __call__(self, images, parameters):\n",
    "        \"\"\"\n",
    "        Takes a batch of images where B (the last dim) is the batch size\n",
    "        args:\n",
    "            images: torch.Tensor # B H W C \n",
    "            parameters :torch.Tensor # N\n",
    "        return:\n",
    "            output: torch.Tensor #  B H W C \n",
    "        \"\"\"\n",
    "        assert images.dim()==4\n",
    "        batch_size = parameters.shape[0]\n",
    "        output = []\n",
    "        for image_index in range(batch_size):\n",
    "            image = images[image_index].numpy()\n",
    "            scale = max((image.shape[:2])) / 512.0\n",
    "            omega = float(parameters[image_index])\n",
    "            editted= dehaze.DarkPriorChannelDehaze(\n",
    "                wsize=int(15*scale), radius=int(80*scale), omega=omega,\n",
    "                t_min=0.25, refine=True)(image * 255.0) / 255.0\n",
    "            editted = torch.tensor(editted)\n",
    "            editted = F.relu(editted)\n",
    "            editted= 1-F.relu(1-editted)\n",
    "            output.append(editted)\n",
    "        output = torch.stack(output)\n",
    "        return output\n",
    "    \n",
    "    \n",
    "new = AdjustDehaze()(input,parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.True_"
      ]
     },
     "execution_count": 371,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(old==new.numpy()[0]).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 448,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "class AdjustClaritye():\n",
    "    def __init__(self):\n",
    "        self.num_parameters = 1\n",
    "        self.window_names = [\"parameter\"]\n",
    "        self.slider_names = [\"clarity\"]\n",
    "\n",
    "    def __call__(self, list_editted, parameters):\n",
    "        editted = list_editted.numpy()\n",
    "        scale = max((editted.shape[:2])) / 512.0\n",
    "        clarity = parameters[0]\n",
    "\n",
    "        unsharped = cv2.bilateralFilter((editted*255.0).astype(np.uint8),\n",
    "                                            int(32*scale), 50, 10*scale)/255.0\n",
    "        editted_ = editted + (editted-unsharped) * clarity\n",
    "        return [editted_]\n",
    "    \n",
    "old = AdjustClaritye()(a,[0.5])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 453,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class AdjustClarity():\n",
    "    def __init__(self):\n",
    "        self.num_parameters = 1\n",
    "        self.window_names = [\"parameter\"]\n",
    "        self.slider_names = [\"clarity\"]\n",
    "\n",
    "    def __call__(self, images, parameters):\n",
    "        \"\"\"\n",
    "        Takes a batch of images where B (the last dim) is the batch size\n",
    "        args:\n",
    "            images: torch.Tensor # B H W C \n",
    "            parameters :torch.Tensor # N\n",
    "        return:\n",
    "            output: torch.Tensor #  B H W C \n",
    "        \"\"\"\n",
    "        assert images.dim()==4\n",
    "        batch_size = parameters.shape[0]\n",
    "        output = [] \n",
    "        clarity = parameters.view(batch_size, 1, 1, 1)\n",
    "        for image in images: \n",
    "            input = image.numpy()      \n",
    "            scale = max((input.shape[:2])) / 512.0\n",
    "            unsharped = cv2.bilateralFilter((input*255.0).astype(np.uint8),\n",
    "                                                int(32*scale), 50, 10*scale)/255.0\n",
    "            output.append(torch.tensor(unsharped))\n",
    "        output = torch.stack(output) \n",
    "        editted_images = images + (images-output) * clarity\n",
    "        \n",
    "        return editted_images\n",
    "    \n",
    "# class AdjustClarity():\n",
    "#     def __init__(self):\n",
    "#         self.num_parameters = 1\n",
    "#         self.window_names = [\"parameter\"]\n",
    "#         self.slider_names = [\"clarity\"]\n",
    "\n",
    "#     def __call__(self, images, parameters):\n",
    "#         assert images.dim()==4\n",
    "#         batch_size = parameters.shape[0]\n",
    "#         output = [] \n",
    "        \n",
    "#         for image_index,image in enumerate(images): \n",
    "#             clarity = float(parameters[image_index])\n",
    "#             input = image.numpy()      \n",
    "#             scale = max((input.shape[:2])) / 512.0\n",
    "#             unsharped = cv2.bilateralFilter((input*255.0).astype(np.uint8),\n",
    "#                                                 int(32*scale), 50, 10*scale)/255.0\n",
    "#             editted = input + (input-unsharped) * clarity\n",
    "#             output.append(torch.tensor(editted))\n",
    "\n",
    "#         output = torch.stack(output) \n",
    "\n",
    "#         return output\n",
    "new = AdjustClarity()(input,parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 455,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.True_"
      ]
     },
     "execution_count": 455,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(old==new.numpy()[1]).all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# exposure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 636,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid_inverse(y):\n",
    "    epsilon = 10**(-3)\n",
    "    y = F.relu(y-epsilon)+epsilon\n",
    "    y = 1-epsilon-F.relu((1-epsilon)-y)\n",
    "    y = (1/y)-1\n",
    "    output = -np.log(y.numpy())\n",
    "    return torch.tensor(output)\n",
    "\n",
    "class SigmoidInverse():\n",
    "\n",
    "    def __init__(self):\n",
    "        self.num_parameters = 0\n",
    "\n",
    "    def __call__(self, images):\n",
    "        return sigmoid_inverse(images)\n",
    "new_sig_inv = SigmoidInverse()\n",
    "\n",
    "def old_sigmoid_inverse(y):\n",
    "    epsilon = 10**(-3)\n",
    "    y_ = y.copy()\n",
    "    y_ = relu(y_-epsilon)+epsilon\n",
    "    y_ = 1-epsilon-relu((1-epsilon)-y_)\n",
    "    y_ = (1/y_)-1\n",
    "    output = -np.log(y_)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 637,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.True_"
      ]
     },
     "execution_count": 637,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(old_sigmoid_inverse(a.numpy())==new_sig_inv (input)[0].numpy()).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 639,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdjustExposuree():\n",
    "    def __init__(self):\n",
    "        self.num_parameters = 1\n",
    "        self.window_names = [\"parameter\"]\n",
    "        self.slider_names = [\"exposure\"]\n",
    "\n",
    "    def __call__(self, list_sigmoid_inversed, parameters):\n",
    "        exposure = parameters[0]\n",
    "        return [old_sigmoid_inverse(list_sigmoid_inversed)+ exposure*5]\n",
    "old = AdjustExposuree()(a.numpy(),[0.5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 640,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdjustExposure():\n",
    "    def __init__(self):\n",
    "        self.num_parameters = 1\n",
    "        self.window_names = [\"parameter\"]\n",
    "        self.slider_names = [\"exposure\"]\n",
    "\n",
    "    def __call__(self, images, parameters):\n",
    "        batch_size = parameters.shape[0]\n",
    "        exposure = parameters.view(batch_size, 1, 1, 1)\n",
    "        output = images+exposure*5\n",
    "        output = new_sig_inv(output)\n",
    "        return output\n",
    "new = AdjustExposure()(input,parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 641,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.True_"
      ]
     },
     "execution_count": 641,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(old_sigmoid_inverse(a.numpy())==new_sig_inv (input)[0].numpy()).all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 770,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdjustTempe():\n",
    "    def __init__(self):\n",
    "        self.num_parameters = 1\n",
    "        self.window_names = [\"parameter\"]\n",
    "        self.slider_names = [\"temp\"]\n",
    "\n",
    "    def __call__(self, list_sigmoid_inversed, parameters):\n",
    "        temp = parameters[0]\n",
    "        sigmoid_inversed_ = list_sigmoid_inversed.copy()\n",
    "        if temp > 0:\n",
    "            sigmoid_inversed_[:,:,1] += temp*1.6\n",
    "            sigmoid_inversed_[:,:,2] += temp*2\n",
    "        else:\n",
    "            sigmoid_inversed_[:,:,0] -= temp*2.0\n",
    "            sigmoid_inversed_[:,:,1] -= temp*1.0\n",
    "        return [sigmoid_inversed_]\n",
    "old = AdjustTempe()(a.numpy(),[0.5])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 773,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdjustTemp():\n",
    "    def __init__(self):\n",
    "        self.num_parameters = 1\n",
    "        self.window_names = [\"parameter\"]\n",
    "        self.slider_names = [\"temp\"]\n",
    "\n",
    "    def __call__(self, images, parameters):\n",
    "        batch_size = parameters.shape[0]\n",
    "        temp = parameters.view(batch_size, 1, 1, 1)\n",
    "        editted = torch.clone(images)  \n",
    "\n",
    "        index_high = (temp>0).view(-1)\n",
    "        index_low = (temp<=0).view(-1)\n",
    "\n",
    "        editted[index_high,:,:,1] += temp[index_high,:,:,0]*1.6\n",
    "        editted[index_high,:,:,2] += temp[index_high,:,:,0]*2   \n",
    "        editted[index_low,:,:,0] -= temp[index_low,:,:,0]*2.0\n",
    "        editted[index_low,:,:,1] -= temp[index_low,:,:,0]*1.0          \n",
    "\n",
    "        return editted\n",
    "    \n",
    "new = AdjustTemp()(input,parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 774,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.True_"
      ]
     },
     "execution_count": 774,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(old==new.numpy()[1]).all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 807,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdjustTinte():\n",
    "    def __init__(self):\n",
    "        self.num_parameters = 1\n",
    "        self.window_names = [\"parameter\"]\n",
    "        self.slider_names = [\"tint\"]\n",
    "\n",
    "    def __call__(self, list_sigmoid_inversed, parameters):\n",
    "        tint = parameters[0]\n",
    "        sigmoid_inversed_ = list_sigmoid_inversed.copy()\n",
    "        if tint > 0:\n",
    "            sigmoid_inversed_[:,:,0] += tint*2\n",
    "            sigmoid_inversed_[:,:,2] += tint*1\n",
    "        else:\n",
    "            sigmoid_inversed_[:,:,1] -= tint*2\n",
    "            sigmoid_inversed_[:,:,2] -= tint*1\n",
    "        return [sigmoid_inversed_]\n",
    "    \n",
    "old = AdjustTinte()(a.numpy(),[0.5])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 808,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdjustTint():\n",
    "    def __init__(self):\n",
    "        self.num_parameters = 1\n",
    "        self.window_names = [\"parameter\"]\n",
    "        self.slider_names = [\"tint\"]\n",
    "\n",
    "    def __call__(self, images, parameters):\n",
    "        batch_size = parameters.shape[0]\n",
    "        tint = parameters.view(batch_size, 1, 1, 1)\n",
    "        editted = torch.clone(images)  \n",
    "\n",
    "        index_high = (tint>0).view(-1)\n",
    "        index_low = (tint<=0).view(-1)\n",
    "\n",
    "        editted[index_high,:,:,0] += tint[index_high,:,:,0]*2\n",
    "        editted[index_high,:,:,2] += tint[index_high,:,:,0]*1  \n",
    "        editted[index_low,:,:,1] -= tint[index_low,:,:,0]*2\n",
    "        editted[index_low,:,:,2] -= tint[index_low,:,:,0]*1         \n",
    "\n",
    "        return editted\n",
    "    \n",
    "new = AdjustTint()(input,parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 809,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.True_"
      ]
     },
     "execution_count": 809,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(old==new.numpy()[1]).all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BGR2HSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1144,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Bgr2Hsve():\n",
    "    def __init__(self):\n",
    "        self.num_parameters = 0\n",
    "\n",
    "    def __call__(self, list_editted, parameters):\n",
    "        editted = list_editted\n",
    "\n",
    "        max_bgr = editted.max(axis=2)\n",
    "        min_bgr = editted.min(axis=2)\n",
    "\n",
    "        b_g = editted[:,:,0]-editted[:,:,1]\n",
    "        g_r = editted[:,:,1]-editted[:,:,2]\n",
    "        r_b = editted[:,:,2]-editted[:,:,0]\n",
    "\n",
    "        b_min_flg = (1-relu(np.sign(b_g)))*relu(np.sign(r_b))\n",
    "        g_min_flg = (1-relu(np.sign(g_r)))*relu(np.sign(b_g))\n",
    "        r_min_flg = (1-relu(np.sign(r_b)))*relu(np.sign(g_r))\n",
    "\n",
    "        epsilon = 10**(-5)\n",
    "        h1 = 60*g_r/(max_bgr-min_bgr+epsilon)+60\n",
    "        h2 = 60*b_g/(max_bgr-min_bgr+epsilon)+180\n",
    "        h3 = 60*r_b/(max_bgr-min_bgr+epsilon)+300\n",
    "        h = h1*b_min_flg + h2*r_min_flg + h3*g_min_flg\n",
    "\n",
    "        v = max_bgr\n",
    "        s = (max_bgr-min_bgr)/(max_bgr+epsilon)\n",
    "\n",
    "        return [h,s,v]\n",
    "old = Bgr2Hsve()(a.numpy(),[0.5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1145,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Bgr2Hsv:\n",
    "    def __init__(self):\n",
    "        self.num_parameters = 0\n",
    "\n",
    "    def __call__(self, images ,parameters=None):\n",
    "        editted = images\n",
    "\n",
    "        max_bgr, _ = editted.max(dim=-1, keepdim=True)\n",
    "        min_bgr, _ = editted.min(dim=-1, keepdim=True)\n",
    "\n",
    "        b = editted[..., 0]\n",
    "        g = editted[..., 1]\n",
    "        r = editted[..., 2]\n",
    "\n",
    "        b_g = b - g\n",
    "        g_r = g - r\n",
    "        r_b = r - b\n",
    "\n",
    "        b_min_flg = (1 - F.relu(torch.sign(b_g))) * F.relu(torch.sign(r_b))\n",
    "        g_min_flg = (1 - F.relu(torch.sign(g_r))) * F.relu(torch.sign(b_g))\n",
    "        r_min_flg = (1 - F.relu(torch.sign(r_b))) * F.relu(torch.sign(g_r))\n",
    "\n",
    "        epsilon = 10**(-5)\n",
    "        h1 = 60 * g_r / (max_bgr.squeeze() - min_bgr.squeeze() + epsilon) + 60\n",
    "        h2 = 60 * b_g / (max_bgr.squeeze() - min_bgr.squeeze() + epsilon) + 180\n",
    "        h3 = 60 * r_b / (max_bgr.squeeze() - min_bgr.squeeze() + epsilon) + 300\n",
    "        h = h1 * b_min_flg + h2 * r_min_flg + h3 * g_min_flg\n",
    "\n",
    "        v = max_bgr.squeeze()\n",
    "        s = (max_bgr.squeeze() - min_bgr.squeeze()) / (max_bgr.squeeze() + epsilon)\n",
    "\n",
    "        return [h, s, v]\n",
    "    \n",
    "new = Bgr2Hsv()(input,parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1146,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert (old[1]==new[1].numpy()).all()\n",
    "assert (old[2]==new[2].numpy()).all()\n",
    "assert (old[0]==new[0].numpy()).all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Shadows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1147,
   "metadata": {},
   "outputs": [],
   "source": [
    "h,s,v = torch.clone(new[0]),torch.clone(new[1]),torch.clone(new[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1158,
   "metadata": {},
   "outputs": [],
   "source": [
    "def numpy_sigmoid(x):\n",
    "    return 1/(1+np.exp(-x))\n",
    "\n",
    "class AdjustShadowse():\n",
    "    def __init__(self):\n",
    "        self.num_parameters = 1\n",
    "        self.window_names = [\"parameter\"]\n",
    "        self.slider_names = [\"shadows\"]\n",
    "\n",
    "    def __call__(self, list_hsv, parameters):\n",
    "        shadows = parameters[0]\n",
    "        v = list_hsv[2]\n",
    "        shadows_mask = 1-numpy_sigmoid((v-0)*5.0)\n",
    "        return [list_hsv[0], list_hsv[1], v*(1+shadows_mask*shadows*5.0)],shadows_mask\n",
    "    \n",
    "old,o = AdjustShadowse()([h.numpy(),s.numpy(),v.numpy()],[0.5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1159,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdjustShadows:\n",
    "    def __init__(self):\n",
    "        self.num_parameters = 1\n",
    "        self.window_names = [\"parameter\"]\n",
    "        self.slider_names = [\"shadows\"]\n",
    "    \n",
    "    def __call__(self, list_hsv, parameters):\n",
    "        batch_size = parameters.shape[0]\n",
    "        shadows = parameters.view(batch_size, 1, 1).numpy()\n",
    "\n",
    "        v = list_hsv[2].numpy()\n",
    "        \n",
    "        # Calculate shadows mask\n",
    "\n",
    "        shadows_mask = 1 - numpy_sigmoid((v - 0.0) * 5.0)\n",
    "        # Adjust v channel based on shadows mask\n",
    "        adjusted_v = v * (1 + shadows_mask * shadows * 5.0)\n",
    "        adjusted_v = torch.tensor(adjusted_v)\n",
    "        return [list_hsv[0], list_hsv[1], adjusted_v],shadows_mask\n",
    "\n",
    "new,n = AdjustShadows()([h,s,v],parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1160,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert (old[0]==new[0].numpy()).all()\n",
    "assert (old[1]==new[1].numpy()).all()\n",
    "assert (old[2]==new[2].numpy()).all()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "photoen",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
