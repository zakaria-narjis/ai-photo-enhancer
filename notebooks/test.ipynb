{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from envs.edit_photo import PhotoEditor\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('dataset/trainTarget.txt') as f :\n",
    "    tif_images = f.read()\n",
    "    jpg_images = tif_images.replace('tif','jpg')\n",
    "with open('dataset/trainTarget_jpg.txt','w+') as f:\n",
    "    f.write(jpg_images)\n",
    "    \n",
    "with open('dataset/trainSource.txt') as f :\n",
    "    tif_images = f.read()\n",
    "    jpg_images = tif_images.replace('tif','jpg')\n",
    "with open('dataset/trainSource_jpg.txt','w+') as f:\n",
    "    f.write(jpg_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = cv2.imread(\"sample_images/a0676-kme_609_original.jpg\")\n",
    "target = cv2.imread(\"sample_images/a0676-kme_609_C.jpg\")\n",
    "image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)/255.0\n",
    "target = cv2.cvtColor(target, cv2.COLOR_BGR2RGB)/255.0\n",
    "photo_editor = PhotoEditor()\n",
    "parameters = np.array([0.125, 0.125, 0.375, 0.125, 0., 0.0625, 0.9375, 0.375, 0.0625, 0., 0.125, 0.125])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from PIL import Image\n",
    "\n",
    "# def resize_length(image_array, size=512):\n",
    "#     \"\"\"\n",
    "#     Resize the longer side of the image to the specified size while maintaining the aspect ratio.\n",
    "\n",
    "#     :param image_array: NumPy array representing the image.\n",
    "#     :param size: The target size for the longer side of the image.\n",
    "#     :return: Resized image as a NumPy array.\n",
    "#     \"\"\"\n",
    "#     image = Image.fromarray(image_array)\n",
    "#     original_width, original_height = image.size\n",
    "#     if original_width > original_height:\n",
    "#         new_width = size\n",
    "#         new_height = int((original_height / original_width) * size)\n",
    "#     else:\n",
    "#         new_height = size\n",
    "#         new_width = int((original_width / original_height) * size)\n",
    "#     resized_image = image.resize((new_width, new_height), Image.LANCZOS)\n",
    "#     resized_image_array = np.array(resized_image)\n",
    "    \n",
    "#     return resized_image_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input = resize_length(image)\n",
    "# input = input / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = photo_editor(image.copy(),parameters.copy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "enhanced = torch.Tensor(output)\n",
    "enhanced_target = torch.Tensor(target)\n",
    "enhanced =torch.flatten(enhanced.clone(),start_dim=0, end_dim=-1)\n",
    "enhanced_target = torch.flatten(enhanced_target.clone(),start_dim=0, end_dim=-1)\n",
    "t= enhanced_target.clone()\n",
    "t[0]=0\n",
    "rmse =  enhanced-enhanced_target\n",
    "rmse = torch.pow(rmse,2).mean()\n",
    "rmse = torch.sqrt(rmse)\n",
    "\n",
    "rewards = -rmse\n",
    "psnr = 20 * torch.log10(1/ rmse) \n",
    "psnr = psnr -100\n",
    "psnr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.Tensor([True,False]).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchrl.data import TensorDictReplayBuffer,LazyMemmapStorage\n",
    "from torchrl.data.replay_buffers.samplers import SamplerWithoutReplacement\n",
    "from tensordict import TensorDict\n",
    "\n",
    "# Check if CUDA is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Define buffer parameters\n",
    "buffer_size = 9000  # example buffer size\n",
    "\n",
    "# Initialize the replay buffer on the GPU\n",
    "replay_buffer = TensorDictReplayBuffer(\n",
    "                        storage=LazyMemmapStorage(buffer_size,), sampler=SamplerWithoutReplacement()\n",
    "                                                        )\n",
    "\n",
    "# Populate the buffer with sample data\n",
    "# Example of how to add data to the buffer\n",
    "for _ in range(buffer_size):\n",
    "    # Create a sample data point\n",
    "    state = torch.randn((32,512,), device=device)  # Replace state_size with actual size\n",
    "    action = torch.randn((32,15,), device=device).clamp(-1, 1)  # 15-dimensional continuous action\n",
    "    reward = torch.randn(32, device=device)\n",
    "    next_state = torch.randn((32,512,), device=device)  # Replace state_size with actual size\n",
    "    done = torch.randn(32, device=device)  # Example done flag\n",
    "    batch_transition = TensorDict(\n",
    "            {\n",
    "                \"observations\":state.clone(),\n",
    "                \"next_observations\":next_state.clone(),\n",
    "                \"actions\":action.clone(),\n",
    "                \"rewards\":reward.clone(),\n",
    "                \"dones\":done.clone(),\n",
    "            },\n",
    "            batch_size = 32,\n",
    "        )\n",
    "    # Add to the replay buffer\n",
    "    replay_buffer.add(batch_transition)\n",
    "\n",
    "# Verify buffer storage on GPU\n",
    "sample = replay_buffer.sample(1)  # Sample a single transition\n",
    "print(sample['state'].device)  # Should output: cuda:0\n",
    "print(sample['action'].device)  # Should output: cuda:0\n",
    "print(sample['reward'].device)  # Should output: cuda:0\n",
    "print(sample['next_state'].device)  # Should output: cuda:0\n",
    "print(sample['done'].device)  #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torcheval.metrics import PeakSignalNoiseRatio\n",
    "metric = PeakSignalNoiseRatio()\n",
    "metric.update(enhanced.permute(2,0,1), enhanced_target.permute(2,0,1))\n",
    "metric.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision.io import read_image\n",
    "import torchvision.transforms.v2.functional as F\n",
    "import os\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from transformers import BertTokenizer, BertModel, CLIPProcessor, CLIPModel\n",
    "from tqdm import tqdm\n",
    "import kornia.color as K\n",
    "class FiveKDataset(Dataset):\n",
    "    def __init__(self, image_size, mode=\"train\", resize=True, \n",
    "                 augment_data=False, use_txt_features=False, device='cuda',\n",
    "                 pre_load_images=True):\n",
    "        current_dir = os.getcwd()\n",
    "        dataset_dir = os.path.join(current_dir,\"..\",\"dataset\")\n",
    "        self.IMGS_PATH = os.path.join(dataset_dir, f\"FiveK/{mode}\")\n",
    "        self.FEATURES_PATH = os.path.join(dataset_dir, \"processed_categories_2.txt\")\n",
    "        self.resize = resize\n",
    "        self.image_size = image_size\n",
    "        self.augment_data = augment_data\n",
    "        self.use_txt_features = use_txt_features\n",
    "        self.device = device\n",
    "        self.pre_load_images = pre_load_images\n",
    "        self.feature_categories = [\"Location\", \"Time\", \"Light\", \"Subject\"]\n",
    "        \n",
    "        # Load semantic features from processed_categories.txt\n",
    "        self.features = {}\n",
    "        with open(self.FEATURES_PATH, 'r') as f:\n",
    "            for line in f:\n",
    "                parts = line.strip().split(',')\n",
    "                img_name = parts[0]\n",
    "                img_features = parts[1:]\n",
    "                self.features[img_name] = img_features\n",
    "        \n",
    "        # Load image files\n",
    "        self.img_files = [f for f in os.listdir(os.path.join(self.IMGS_PATH, 'input'))]\n",
    "        \n",
    "        # Prepare MultiLabelBinarizer\n",
    "        all_features = [self.features[img] for img in self.img_files]\n",
    "        self.mlb = MultiLabelBinarizer()\n",
    "        self.mlb.fit(all_features)\n",
    "        \n",
    "        # Create encoding dictionaries for categorical approach\n",
    "        unique_features = {cat: set(feat[i] for feat in all_features) \n",
    "                           for i, cat in enumerate(self.feature_categories)}\n",
    "        self.feature_to_idx = {\n",
    "            cat: {feat: idx for idx, feat in enumerate(sorted(features))}\n",
    "            for cat, features in unique_features.items()\n",
    "        }\n",
    "        if self.use_txt_features == \"histogram\":\n",
    "            self.histogram_bins = 64       \n",
    "        # Preload all images and features if pre_load_images is True\n",
    "        if self.pre_load_images:\n",
    "            self.preload_data()\n",
    "        \n",
    "        if self.use_txt_features == \"embedded\":\n",
    "            self.precompute_features()\n",
    "\n",
    "    def compute_histogram(self, image):\n",
    "        # Convert RGB to CIE Lab\n",
    "        lab_image = K.rgb_to_lab(image.unsqueeze(0)/255.0).squeeze(0)\n",
    "        \n",
    "        # Compute histogram for each channel\n",
    "        histograms = []\n",
    "        for channel in range(3):  # L, a, b channels\n",
    "            if channel == 0:  # L channel\n",
    "                hist = torch.histc(lab_image[channel], bins=self.histogram_bins, min=0, max=100)\n",
    "            else:  # a and b channels\n",
    "                hist = torch.histc(lab_image[channel], bins=self.histogram_bins, min=-128, max=127)\n",
    "            # Normalize the histogram\n",
    "            hist = hist / hist.sum()\n",
    "            histograms.append(hist)\n",
    "        \n",
    "        # Concatenate histograms\n",
    "        return torch.cat(histograms)\n",
    "    \n",
    "    def preload_data(self):\n",
    "        print(\"Preloading images and features...\")\n",
    "        self.source_images = []\n",
    "        self.target_images = []\n",
    "        self.one_hot_features = []\n",
    "        self.cat_features = []\n",
    "        self.histograms = []\n",
    "        \n",
    "        for img_name in tqdm(self.img_files):\n",
    "            # Load and preprocess images\n",
    "            source = read_image(os.path.join(self.IMGS_PATH, 'input', img_name))\n",
    "            target = read_image(os.path.join(self.IMGS_PATH, 'target', img_name))\n",
    "            \n",
    "            if self.resize:\n",
    "                source = F.resize(source, (self.image_size, self.image_size), interpolation=F.InterpolationMode.BICUBIC)\n",
    "                target = F.resize(target, (self.image_size, self.image_size), interpolation=F.InterpolationMode.BICUBIC)\n",
    "            \n",
    "            self.source_images.append(source.to(self.device))\n",
    "            self.target_images.append(target.to(self.device))\n",
    "            \n",
    "            # Precompute features\n",
    "            if self.use_txt_features == \"one_hot\":\n",
    "                one_hot = self.mlb.transform([self.features[img_name]])[0]\n",
    "                self.one_hot_features.append(torch.tensor(one_hot, dtype=torch.float32, device=self.device))\n",
    "            elif self.use_txt_features == \"categorical\":\n",
    "                cat = [self.feature_to_idx[cat][feat] for cat, feat in zip(self.feature_categories, self.features[img_name])]\n",
    "                self.cat_features.append(torch.tensor(cat, dtype=torch.long, device=self.device))\n",
    "            elif self.use_txt_features == \"histogram\":\n",
    "                source_hist = self.compute_histogram(source).to(self.device)\n",
    "                target_hist = self.compute_histogram(target).to(self.device)\n",
    "                self.histograms.append((source_hist, target_hist))\n",
    "        \n",
    "        self.source_images = torch.stack(self.source_images)\n",
    "        self.target_images = torch.stack(self.target_images)\n",
    "        \n",
    "        if self.use_txt_features == \"one_hot\":\n",
    "            self.one_hot_features = torch.stack(self.one_hot_features)\n",
    "        elif self.use_txt_features == \"categorical\":\n",
    "            self.cat_features = torch.stack(self.cat_features)\n",
    "        elif self.use_txt_features == \"histogram\":\n",
    "            self.source_histograms = torch.stack([h[0] for h in self.histograms])\n",
    "            self.target_histograms = torch.stack([h[1] for h in self.histograms])\n",
    "        \n",
    "        print(\"Images and features preloaded and stored in GPU memory.\")\n",
    "\n",
    "    def precompute_features(self):\n",
    "        print(\"Precomputing BERT and CLIP features...\")\n",
    "        self.bert_features = []\n",
    "        self.clip_features = []\n",
    "        \n",
    "        tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "        bert_model = BertModel.from_pretrained('bert-base-uncased').to(self.device).eval()\n",
    "        clip_processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "        clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\").to(self.device).eval()\n",
    "        \n",
    "        for img_name in tqdm(self.img_files):\n",
    "            # Precompute BERT features\n",
    "            feature_text = \" \".join(self.features[img_name])\n",
    "            inputs = tokenizer(feature_text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512).to(self.device)\n",
    "            with torch.no_grad():              \n",
    "                outputs = bert_model(**inputs)\n",
    "            bert_features = outputs.last_hidden_state[:, 0, :].squeeze(0)  # Shape: (768,)\n",
    "            self.bert_features.append(bert_features)\n",
    "\n",
    "            # Precompute CLIP features\n",
    "            image = self.source_images[len(self.bert_features) - 1].cpu()  # Get the corresponding preloaded image\n",
    "            clip_inputs = clip_processor(images=image, return_tensors=\"pt\").to(self.device)\n",
    "            with torch.no_grad():           \n",
    "                clip_features = clip_model.get_image_features(**clip_inputs)\n",
    "            self.clip_features.append(clip_features.squeeze(0))  # Shape: (512,)\n",
    "        \n",
    "        self.bert_features = torch.stack(self.bert_features).to(self.device)\n",
    "        self.clip_features = torch.stack(self.clip_features).to(self.device)\n",
    "        \n",
    "        del bert_model, tokenizer, clip_model, clip_processor\n",
    "        print(\"BERT and CLIP features precomputed and stored in GPU memory.\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if self.pre_load_images:\n",
    "            source = self.source_images[idx]\n",
    "            target = self.target_images[idx]\n",
    "        else:\n",
    "            img_name = self.img_files[idx]\n",
    "            source = read_image(os.path.join(self.IMGS_PATH, 'input', img_name))\n",
    "            target = read_image(os.path.join(self.IMGS_PATH, 'target', img_name))\n",
    "            \n",
    "            if self.resize:\n",
    "                source = F.resize(source, (self.image_size, self.image_size), interpolation=F.InterpolationMode.BICUBIC)\n",
    "                target = F.resize(target, (self.image_size, self.image_size), interpolation=F.InterpolationMode.BICUBIC)\n",
    "            \n",
    "            source = source.to(self.device)\n",
    "            target = target.to(self.device)\n",
    "\n",
    "        if self.augment_data:\n",
    "            if torch.rand(1).item() > 0.5:\n",
    "                source = F.hflip(source)\n",
    "                target = F.hflip(target)\n",
    "            if torch.rand(1).item() > 0.5:\n",
    "                source = F.vflip(source)\n",
    "                target = F.vflip(target)\n",
    "\n",
    "        if not self.use_txt_features:\n",
    "            return source, target\n",
    "        elif self.use_txt_features == \"one_hot\":\n",
    "            if self.pre_load_images:\n",
    "                return source, self.one_hot_features[idx], target\n",
    "            else:\n",
    "                one_hot = self.mlb.transform([self.features[self.img_files[idx]]])[0]\n",
    "                return source, torch.tensor(one_hot, dtype=torch.float32, device=self.device), target\n",
    "        elif self.use_txt_features == \"categorical\":\n",
    "            if self.pre_load_images:\n",
    "                return source, self.cat_features[idx], target\n",
    "            else:\n",
    "                cat = [self.feature_to_idx[cat][feat] for cat, feat in zip(self.feature_categories, self.features[self.img_files[idx]])]\n",
    "                return source, torch.tensor(cat, dtype=torch.long, device=self.device), target\n",
    "        elif self.use_txt_features == \"embedded\":\n",
    "            return source, self.bert_features[idx], self.clip_features[idx], target\n",
    "        elif self.use_txt_features == \"histogram\":\n",
    "            if self.pre_load_images:\n",
    "                return source, self.source_histograms[idx], target, self.target_histograms[idx]\n",
    "            else:\n",
    "                source_hist = self.compute_histogram(source).to(self.device)\n",
    "                target_hist = self.compute_histogram(target).to(self.device)\n",
    "                return source, source_hist, target, target_hist\n",
    "        else:\n",
    "            raise ValueError(\"Invalid value for use_txt_features. Must be False, 'one_hot', 'categorical', 'embedded', or 'histogram'.\")\n",
    "\n",
    "    def collate_fn(self, batch):\n",
    "        if self.use_txt_features == \"embedded\":\n",
    "            sources, bert_features, clip_features, targets = zip(*batch)\n",
    "            return torch.stack(sources), torch.stack(bert_features), torch.stack(clip_features), torch.stack(targets)\n",
    "        elif self.use_txt_features == \"histogram\":\n",
    "            sources, source_hists, targets, target_hists = zip(*batch)\n",
    "            return torch.stack(sources), torch.stack(source_hists), torch.stack(targets), torch.stack(target_hists)\n",
    "        else:\n",
    "            sources, features, targets = zip(*batch)\n",
    "            return torch.stack(sources), torch.stack(features) if features[0] is not None else None, torch.stack(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f=FiveKDataset(64, mode=\"train\", resize=True, augment_data=False, use_txt_features=\"histogram\", device='cuda', pre_load_images=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f[0][1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "64*3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(19.7316)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from PIL import Image\n",
    "import time\n",
    "from torchvision.io import read_image\n",
    "img_name = \"a0002-dgw_005.jpg\"\n",
    "original_image = read_image(f\"../dataset/FiveK/train/input/{img_name}\")/255.0\n",
    "target_image = read_image(f\"../dataset/FiveK/train/target/{img_name}\")/255.0\n",
    "enhanced = torch.flatten(\n",
    "    original_image.clone(), start_dim=0, end_dim=-1\n",
    ")\n",
    "target = torch.flatten(target_image.clone(), start_dim=0, end_dim=-1)\n",
    "\n",
    "rmse = enhanced - target\n",
    "rmse = torch.pow(rmse, 2).mean()\n",
    "rmse = torch.sqrt(rmse)\n",
    "psnr = (20 * torch.log10(1 / rmse))\n",
    "rewards = psnr\n",
    "print(rewards)\n",
    "import os\n",
    "os.chdir('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.envs.new_edit_photo import PhotoEditor\n",
    "original_image = read_image(\"/home/zakaria/workspace/ai-photo-enhancer/src/sample_images/a0002-dgw_005.jpg\")/255.0\n",
    "device = 'cpu'\n",
    "source_image = original_image.to(device).permute(1,2,0).unsqueeze(0)\n",
    "parameters = torch.rand((1,10),device=device)\n",
    "sliders = ['temp', 'tint', 'vibrance', 'saturation', 'contrast', 'exposure', 'shadows', 'highlights', 'whites', 'blacks']\n",
    "photo_editor = PhotoEditor(sliders=sliders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_image.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.531782865524292\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "enhanced = photo_editor(source_image,parameters)\n",
    "end_time = time.time()\n",
    "print(end_time-start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sat Aug 10 18:24:20 2024    enhancement_stats\n",
      "\n",
      "         373 function calls in 10.100 seconds\n",
      "\n",
      "   Ordered by: cumulative time\n",
      "   List reduced from 100 to 20 due to restriction <20>\n",
      "\n",
      "   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n",
      "        1    0.000    0.000   10.100   10.100 {built-in method builtins.exec}\n",
      "        1    0.025    0.025   10.100   10.100 <string>:1(<module>)\n",
      "        1    0.085    0.085   10.075   10.075 /tmp/ipykernel_147446/2466602610.py:4(run_enhancement)\n",
      "        1    0.231    0.231    9.921    9.921 /home/zakaria/workspace/ai-photo-enhancer/src/envs/new_edit_photo.py:699(__call__)\n",
      "        1    2.749    2.749    3.096    3.096 /home/zakaria/workspace/ai-photo-enhancer/src/envs/new_edit_photo.py:617(__call__)\n",
      "        1    1.678    1.678    2.540    2.540 /home/zakaria/workspace/ai-photo-enhancer/src/envs/new_edit_photo.py:365(__call__)\n",
      "        1    0.795    0.795    1.382    1.382 /home/zakaria/workspace/ai-photo-enhancer/src/envs/new_edit_photo.py:501(__call__)\n",
      "       32    0.000    0.000    0.915    0.029 /home/zakaria/miniconda3/envs/photoen/lib/python3.11/site-packages/torch/_tensor.py:34(wrapped)\n",
      "        1    0.523    0.523    0.807    0.807 /home/zakaria/workspace/ai-photo-enhancer/src/envs/new_edit_photo.py:288(__call__)\n",
      "       30    0.520    0.017    0.520    0.017 {built-in method torch.relu}\n",
      "        1    0.009    0.009    0.500    0.500 /home/zakaria/workspace/ai-photo-enhancer/src/envs/new_edit_photo.py:33(__call__)\n",
      "        1    0.235    0.235    0.491    0.491 /home/zakaria/workspace/ai-photo-enhancer/src/envs/new_edit_photo.py:11(sigmoid_inverse)\n",
      "       27    0.000    0.000    0.449    0.017 /home/zakaria/miniconda3/envs/photoen/lib/python3.11/site-packages/torch/_tensor.py:964(__rsub__)\n",
      "       27    0.449    0.017    0.449    0.017 {built-in method torch.rsub}\n",
      "        3    0.388    0.129    0.388    0.129 {built-in method torch.cat}\n",
      "        4    0.384    0.096    0.384    0.096 {method 'pow' of 'torch._C.TensorBase' objects}\n",
      "       24    0.359    0.015    0.359    0.015 {built-in method torch.sign}\n",
      "        1    0.175    0.175    0.337    0.337 /home/zakaria/workspace/ai-photo-enhancer/src/envs/new_edit_photo.py:46(__call__)\n",
      "       12    0.000    0.000    0.252    0.021 /home/zakaria/miniconda3/envs/photoen/lib/python3.11/site-packages/torch/nn/functional.py:1489(relu)\n",
      "        1    0.123    0.123    0.167    0.167 /home/zakaria/workspace/ai-photo-enhancer/src/envs/new_edit_photo.py:159(__call__)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<pstats.Stats at 0x71b8b3382a50>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import cProfile\n",
    "import pstats\n",
    "\n",
    "def run_enhancement():\n",
    "    original_image = read_image(\"/home/zakaria/workspace/ai-photo-enhancer/src/sample_images/a0002-dgw_005.jpg\")/255.0\n",
    "    device = 'cpu'\n",
    "    source_image = original_image.to(device).permute(1,2,0).unsqueeze(0)\n",
    "    parameters = torch.rand((1,10),device=device)\n",
    "    sliders = ['temp', 'tint', 'vibrance', 'saturation', 'contrast', 'exposure', 'shadows', 'highlights', 'whites', 'blacks']\n",
    "    photo_editor = PhotoEditor(sliders=sliders)\n",
    "    enhanced = photo_editor(source_image,parameters)\n",
    "\n",
    "cProfile.run('run_enhancement()', 'enhancement_stats')\n",
    "\n",
    "# Print sorted stats\n",
    "p = pstats.Stats('enhancement_stats')\n",
    "p.sort_stats('cumulative').print_stats(20)  # Print top 20 time-consuming functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse = enhanced - source_image\n",
    "rmse = torch.pow(rmse, 2).mean()\n",
    "rmse = torch.sqrt(rmse)\n",
    "psnr = (20 * torch.log10(1 / rmse))\n",
    "psnr "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "photoen",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
