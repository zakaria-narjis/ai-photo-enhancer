{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision.io import read_image\n",
    "import torchvision.transforms.v2.functional as F\n",
    "import os\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from transformers import BertTokenizer, BertModel, CLIPProcessor, CLIPModel\n",
    "from tqdm import tqdm\n",
    "import kornia.color as K\n",
    "class FiveKDataset(Dataset):\n",
    "    def __init__(self, image_size, mode=\"train\", resize=True, \n",
    "                 augment_data=False, use_txt_features=False, device='cuda',\n",
    "                 pre_load_images=True):\n",
    "        current_dir = os.getcwd()\n",
    "        dataset_dir = os.path.join(current_dir,\"..\",\"dataset\")\n",
    "        self.IMGS_PATH = os.path.join(dataset_dir, f\"FiveK_dupplicated/{mode}\")\n",
    "        self.FEATURES_PATH = os.path.join(dataset_dir, \"processed_categories_2.txt\")\n",
    "        self.resize = resize\n",
    "        self.image_size = image_size\n",
    "        self.augment_data = augment_data\n",
    "        self.use_txt_features = use_txt_features\n",
    "        self.device = device\n",
    "        self.pre_load_images = pre_load_images\n",
    "        self.feature_categories = [\"Location\", \"Time\", \"Light\", \"Subject\"]\n",
    "        \n",
    "        # Load semantic features from processed_categories.txt\n",
    "        self.features = {}\n",
    "        with open(self.FEATURES_PATH, 'r') as f:\n",
    "            for line in f:\n",
    "                parts = line.strip().split(',')\n",
    "                img_name = parts[0]\n",
    "                img_features = parts[1:]\n",
    "                self.features[img_name] = img_features\n",
    "        \n",
    "        # Load image files\n",
    "        self.img_files = [f for f in os.listdir(os.path.join(self.IMGS_PATH, 'input'))]\n",
    "        \n",
    "        # Prepare MultiLabelBinarizer\n",
    "        all_features = []\n",
    "        for img in self.img_files:\n",
    "            original_img = img.replace(\"_duplicated\",\"\")\n",
    "            all_features.append(self.features[original_img])\n",
    "     \n",
    "        self.mlb = MultiLabelBinarizer()\n",
    "        self.mlb.fit(all_features)\n",
    "        \n",
    "        # Create encoding dictionaries for categorical approach\n",
    "        unique_features = {cat: set(feat[i] for feat in all_features) \n",
    "                           for i, cat in enumerate(self.feature_categories)}\n",
    "        self.feature_to_idx = {\n",
    "            cat: {feat: idx for idx, feat in enumerate(sorted(features))}\n",
    "            for cat, features in unique_features.items()\n",
    "        }\n",
    "        if self.use_txt_features == \"histogram\":\n",
    "            self.histogram_bins = 64       \n",
    "        # Preload all images and features if pre_load_images is True\n",
    "        if self.pre_load_images:\n",
    "            self.preload_data()\n",
    "        \n",
    "        if self.use_txt_features == \"embedded\":\n",
    "            self.precompute_features()\n",
    "\n",
    "    def compute_histogram(self, image):\n",
    "        # Convert RGB to CIE Lab\n",
    "        lab_image = K.rgb_to_lab(image.unsqueeze(0)/255.0).squeeze(0)\n",
    "        \n",
    "        # Compute histogram for each channel\n",
    "        histograms = []\n",
    "        for channel in range(3):  # L, a, b channels\n",
    "            if channel == 0:  # L channel\n",
    "                hist = torch.histc(lab_image[channel], bins=self.histogram_bins, min=0, max=100)\n",
    "            else:  # a and b channels\n",
    "                hist = torch.histc(lab_image[channel], bins=self.histogram_bins, min=-128, max=127)\n",
    "            # Normalize the histogram\n",
    "            hist = hist / hist.sum()\n",
    "            histograms.append(hist)\n",
    "        \n",
    "        # Concatenate histograms\n",
    "        return torch.cat(histograms)\n",
    "    \n",
    "    def preload_data(self):\n",
    "        print(\"Preloading images and features...\")\n",
    "        self.source_images = []\n",
    "        self.target_images = []\n",
    "        self.one_hot_features = []\n",
    "        self.cat_features = []\n",
    "        self.histograms = []\n",
    "        \n",
    "        for img_name in tqdm(self.img_files):\n",
    "            # Load and preprocess images\n",
    "            source = read_image(os.path.join(self.IMGS_PATH, 'input', img_name))\n",
    "            target = read_image(os.path.join(self.IMGS_PATH, 'target', img_name))\n",
    "            \n",
    "            if self.resize:\n",
    "                source = F.resize(source, (self.image_size, self.image_size), interpolation=F.InterpolationMode.BICUBIC)\n",
    "                target = F.resize(target, (self.image_size, self.image_size), interpolation=F.InterpolationMode.BICUBIC)\n",
    "            \n",
    "            self.source_images.append(source.to(self.device))\n",
    "            self.target_images.append(target.to(self.device))\n",
    "            \n",
    "            # Precompute features\n",
    "            if self.use_txt_features == \"one_hot\":\n",
    "                one_hot = self.mlb.transform([self.features[img_name]])[0]\n",
    "                self.one_hot_features.append(torch.tensor(one_hot, dtype=torch.float32, device=self.device))\n",
    "            elif self.use_txt_features == \"categorical\":\n",
    "                cat = [self.feature_to_idx[cat][feat] for cat, feat in zip(self.feature_categories, self.features[img_name])]\n",
    "                self.cat_features.append(torch.tensor(cat, dtype=torch.long, device=self.device))\n",
    "            elif self.use_txt_features == \"histogram\":\n",
    "                source_hist = self.compute_histogram(source).to(self.device)\n",
    "                target_hist = self.compute_histogram(target).to(self.device)\n",
    "                self.histograms.append((source_hist, target_hist))\n",
    "        \n",
    "        self.source_images = torch.stack(self.source_images)\n",
    "        self.target_images = torch.stack(self.target_images)\n",
    "        \n",
    "        if self.use_txt_features == \"one_hot\":\n",
    "            self.one_hot_features = torch.stack(self.one_hot_features)\n",
    "        elif self.use_txt_features == \"categorical\":\n",
    "            self.cat_features = torch.stack(self.cat_features)\n",
    "        elif self.use_txt_features == \"histogram\":\n",
    "            self.source_histograms = torch.stack([h[0] for h in self.histograms])\n",
    "            self.target_histograms = torch.stack([h[1] for h in self.histograms])\n",
    "        \n",
    "        print(\"Images and features preloaded and stored in GPU memory.\")\n",
    "\n",
    "    def precompute_features(self):\n",
    "        print(\"Precomputing BERT and CLIP features...\")\n",
    "        self.bert_features = []\n",
    "        self.clip_features = []\n",
    "        \n",
    "        tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "        bert_model = BertModel.from_pretrained('bert-base-uncased').to(self.device).eval()\n",
    "        clip_processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "        clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\").to(self.device).eval()\n",
    "        \n",
    "        for img_name in tqdm(self.img_files):\n",
    "            # Precompute BERT features\n",
    "            feature_text = \" \".join(self.features[img_name])\n",
    "            inputs = tokenizer(feature_text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512).to(self.device)\n",
    "            with torch.no_grad():              \n",
    "                outputs = bert_model(**inputs)\n",
    "            bert_features = outputs.last_hidden_state[:, 0, :].squeeze(0)  # Shape: (768,)\n",
    "            self.bert_features.append(bert_features)\n",
    "\n",
    "            # Precompute CLIP features\n",
    "            image = self.source_images[len(self.bert_features) - 1].cpu()  # Get the corresponding preloaded image\n",
    "            clip_inputs = clip_processor(images=image, return_tensors=\"pt\").to(self.device)\n",
    "            with torch.no_grad():           \n",
    "                clip_features = clip_model.get_image_features(**clip_inputs)\n",
    "            self.clip_features.append(clip_features.squeeze(0))  # Shape: (512,)\n",
    "        \n",
    "        self.bert_features = torch.stack(self.bert_features).to(self.device)\n",
    "        self.clip_features = torch.stack(self.clip_features).to(self.device)\n",
    "        \n",
    "        del bert_model, tokenizer, clip_model, clip_processor\n",
    "        print(\"BERT and CLIP features precomputed and stored in GPU memory.\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if self.pre_load_images:\n",
    "            source = self.source_images[idx]\n",
    "            target = self.target_images[idx]\n",
    "        else:\n",
    "            img_name = self.img_files[idx]\n",
    "            source = read_image(os.path.join(self.IMGS_PATH, 'input', img_name))\n",
    "            target = read_image(os.path.join(self.IMGS_PATH, 'target', img_name))\n",
    "            \n",
    "            if self.resize:\n",
    "                source = F.resize(source, (self.image_size, self.image_size), interpolation=F.InterpolationMode.BICUBIC)\n",
    "                target = F.resize(target, (self.image_size, self.image_size), interpolation=F.InterpolationMode.BICUBIC)\n",
    "            \n",
    "            source = source.to(self.device)\n",
    "            target = target.to(self.device)\n",
    "\n",
    "        if self.augment_data:\n",
    "            if torch.rand(1).item() > 0.5:\n",
    "                source = F.hflip(source)\n",
    "                target = F.hflip(target)\n",
    "            if torch.rand(1).item() > 0.5:\n",
    "                source = F.vflip(source)\n",
    "                target = F.vflip(target)\n",
    "\n",
    "        if not self.use_txt_features:\n",
    "            return source, target\n",
    "        elif self.use_txt_features == \"one_hot\":\n",
    "            if self.pre_load_images:\n",
    "                return source, self.one_hot_features[idx], target\n",
    "            else:\n",
    "                one_hot = self.mlb.transform([self.features[self.img_files[idx]]])[0]\n",
    "                return source, torch.tensor(one_hot, dtype=torch.float32, device=self.device), target\n",
    "        elif self.use_txt_features == \"categorical\":\n",
    "            if self.pre_load_images:\n",
    "                return source, self.cat_features[idx], target\n",
    "            else:\n",
    "                cat = [self.feature_to_idx[cat][feat] for cat, feat in zip(self.feature_categories, self.features[self.img_files[idx]])]\n",
    "                return source, torch.tensor(cat, dtype=torch.long, device=self.device), target\n",
    "        elif self.use_txt_features == \"embedded\":\n",
    "            return source, self.bert_features[idx], self.clip_features[idx], target\n",
    "        elif self.use_txt_features == \"histogram\":\n",
    "            if self.pre_load_images:\n",
    "                return source, self.source_histograms[idx], target, self.target_histograms[idx]\n",
    "            else:\n",
    "                source_hist = self.compute_histogram(source).to(self.device)\n",
    "                target_hist = self.compute_histogram(target).to(self.device)\n",
    "                return source, source_hist, target, target_hist\n",
    "        else:\n",
    "            raise ValueError(\"Invalid value for use_txt_features. Must be False, 'one_hot', 'categorical', 'embedded', or 'histogram'.\")\n",
    "\n",
    "    def collate_fn(self, batch):\n",
    "        if self.use_txt_features == \"embedded\":\n",
    "            sources, bert_features, clip_features, targets = zip(*batch)\n",
    "            return torch.stack(sources), torch.stack(bert_features), torch.stack(clip_features), torch.stack(targets)\n",
    "        elif self.use_txt_features == \"histogram\":\n",
    "            sources, source_hists, targets, target_hists = zip(*batch)\n",
    "            return torch.stack(sources), torch.stack(source_hists), torch.stack(targets), torch.stack(target_hists)\n",
    "        else:\n",
    "            sources, features, targets = zip(*batch)\n",
    "            return torch.stack(sources), torch.stack(features) if features[0] is not None else None, torch.stack(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "f=FiveKDataset(64,pre_load_images=False,use_txt_features=\"histogram\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2.4414e-04, 0.0000e+00, 7.3242e-04, 4.8828e-04, 2.4414e-04, 4.8828e-04,\n",
       "        7.3242e-04, 2.4414e-04, 4.8828e-04, 0.0000e+00, 1.4648e-03, 2.1973e-03,\n",
       "        5.8594e-03, 1.0254e-02, 1.3428e-02, 1.7334e-02, 2.7832e-02, 3.2227e-02,\n",
       "        3.2715e-02, 3.6865e-02, 3.8086e-02, 2.0996e-02, 2.4658e-02, 3.3203e-02,\n",
       "        3.5400e-02, 3.3203e-02, 4.0039e-02, 4.5898e-02, 4.5654e-02, 5.3467e-02,\n",
       "        4.7607e-02, 4.6387e-02, 4.7363e-02, 4.6143e-02, 4.1504e-02, 3.6621e-02,\n",
       "        3.1250e-02, 2.8076e-02, 2.5635e-02, 2.4170e-02, 1.5625e-02, 9.7656e-03,\n",
       "        1.1963e-02, 4.6387e-03, 3.9062e-03, 3.1738e-03, 2.9297e-03, 1.9531e-03,\n",
       "        1.7090e-03, 1.4648e-03, 1.4648e-03, 2.4414e-03, 1.9531e-03, 7.3242e-04,\n",
       "        0.0000e+00, 2.6855e-03, 2.4414e-04, 2.1973e-03, 9.7656e-04, 4.8828e-04,\n",
       "        2.4414e-04, 0.0000e+00, 2.4414e-04, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 3.4912e-02,\n",
       "        1.7822e-02, 5.3711e-03, 1.0498e-02, 5.4199e-02, 1.4380e-01, 2.8809e-01,\n",
       "        2.3120e-01, 1.3940e-01, 2.3682e-02, 1.9531e-02, 1.5869e-02, 1.0254e-02,\n",
       "        4.1504e-03, 1.2207e-03, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "        0.0000e+00, 9.7656e-04, 2.1973e-03, 2.9297e-03, 1.3672e-02, 2.5879e-02,\n",
       "        4.1992e-02, 4.3213e-02, 5.5176e-02, 7.0068e-02, 1.0645e-01, 1.6187e-01,\n",
       "        1.3574e-01, 1.1572e-01, 1.0693e-01, 7.1533e-02, 2.1484e-02, 9.0332e-03,\n",
       "        8.5449e-03, 6.1035e-03, 4.8828e-04, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f[4500][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[178, 143, 199,  ...,  94, 106, 101],\n",
       "         [164, 148, 198,  ..., 102, 110, 107],\n",
       "         [134, 161, 194,  ..., 106, 112, 109],\n",
       "         ...,\n",
       "         [167, 177, 169,  ...,   5,  12,  22],\n",
       "         [190, 185, 184,  ...,   8,  16,  22],\n",
       "         [189, 174, 177,  ...,   6,  19,  18]],\n",
       "\n",
       "        [[177, 140, 199,  ...,  99, 113, 108],\n",
       "         [160, 142, 198,  ..., 102, 114, 113],\n",
       "         [124, 153, 191,  ..., 107, 116, 112],\n",
       "         ...,\n",
       "         [167, 172, 162,  ...,   5,  10,  22],\n",
       "         [191, 187, 185,  ...,   6,  16,  24],\n",
       "         [191, 175, 181,  ...,   5,  19,  19]],\n",
       "\n",
       "        [[151, 113, 173,  ...,  52,  56,  54],\n",
       "         [132, 114, 173,  ...,  58,  61,  56],\n",
       "         [ 93, 123, 167,  ...,  63,  62,  58],\n",
       "         ...,\n",
       "         [129, 140, 129,  ...,   2,   5,  14],\n",
       "         [162, 157, 157,  ...,   3,  11,  18],\n",
       "         [160, 143, 152,  ...,   1,  14,  13]]], device='cuda:0',\n",
       "       dtype=torch.uint8)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "def duplicate_images(dataset_dir):\n",
    "    modes = ['train', 'test']\n",
    "    \n",
    "    for mode in modes:\n",
    "        input_dir = os.path.join(dataset_dir, mode, 'input')\n",
    "        target_dir = os.path.join(dataset_dir, mode, 'target')\n",
    "        \n",
    "        # Create input directory if it does not exist\n",
    "        if not os.path.exists(input_dir):\n",
    "            os.makedirs(input_dir)\n",
    "        \n",
    "        # List all files in the target directory\n",
    "        target_files = os.listdir(target_dir)\n",
    "        \n",
    "        for target_file in target_files:\n",
    "            original_target_path = os.path.join(target_dir, target_file)\n",
    "            \n",
    "            # Create duplicated file names\n",
    "            duplicated_file_name = os.path.splitext(target_file)[0] + \"_duplicated\" + os.path.splitext(target_file)[1]\n",
    "            \n",
    "            duplicated_input_path = os.path.join(input_dir, duplicated_file_name)\n",
    "            duplicated_target_path = os.path.join(target_dir, duplicated_file_name)\n",
    "            \n",
    "            # Copy the target image to the input and target folders with the new name\n",
    "            shutil.copy(original_target_path, duplicated_input_path)\n",
    "            shutil.copy(original_target_path, duplicated_target_path)\n",
    "\n",
    "# Set the path to your dataset directory\n",
    "dataset_dir = \"/home/zakaria/workspace/ai-photo-enhancer/dataset/FiveK_dupplicated\"\n",
    "duplicate_images(dataset_dir)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "photoen",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
