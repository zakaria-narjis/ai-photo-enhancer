{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from torchvision.io import read_image\n",
    "import torchvision.transforms.v2.functional as F\n",
    "import random\n",
    "import os\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision.io import read_image\n",
    "import torchvision.transforms.functional as F\n",
    "import random\n",
    "import torch\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from transformers import BertTokenizer, BertModel, CLIPProcessor, CLIPModel\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FiveKDataset(Dataset):\n",
    "    def __init__(self, image_size, mode=\"train\", resize=True, \n",
    "                 augment_data=False, use_txt_features=False, device='cuda',\n",
    "                 test_file=None):\n",
    "        current_dir = os.getcwd()\n",
    "        dataset_dir = os.path.join(current_dir,\"..\",\"dataset\")\n",
    "        self.IMGS_PATH = os.path.join(dataset_dir, \"FiveK\")\n",
    "        self.FEATURES_PATH = os.path.join(dataset_dir, \"processed_categories_2.txt\")\n",
    "        \n",
    "        self.resize = resize\n",
    "        self.image_size = image_size\n",
    "        self.augment_data = augment_data\n",
    "        self.use_txt_features = use_txt_features\n",
    "        self.device = device\n",
    "        self.mode = mode\n",
    "        self.img_files = []\n",
    "        self.features = {}\n",
    "        self.feature_categories = [\"Location\", \"Time\", \"Light\", \"Subject\"]\n",
    "        \n",
    "        # Load semantic features from processed_categories.txt\n",
    "        with open(self.FEATURES_PATH, 'r') as f:\n",
    "            for line in f:\n",
    "                parts = line.strip().split(',')\n",
    "                img_name = parts[0]\n",
    "                img_features = parts[1:]\n",
    "                self.features[img_name] = img_features\n",
    "        \n",
    "        # Load image files\n",
    "        if mode == \"test\" and test_file:\n",
    "            test_file_path = os.path.join(dataset_dir, test_file)\n",
    "            with open(test_file_path, 'r') as f:\n",
    "                self.img_files = [line.strip() for line in f]\n",
    "            self.img_locations = self._locate_test_images()\n",
    "        else:\n",
    "            self.img_files = [f for f in os.listdir(os.path.join(self.IMGS_PATH, mode, 'input'))]\n",
    "        \n",
    "        all_features = [self.features[img] for img in self.img_files]\n",
    "        self.mlb = MultiLabelBinarizer()\n",
    "        self.mlb.fit(all_features)\n",
    "        \n",
    "        # Create encoding dictionaries for categorical approach\n",
    "        unique_features = {cat: set(feat[i] for feat in all_features) \n",
    "                           for i, cat in enumerate(self.feature_categories)}\n",
    "        self.feature_to_idx = {\n",
    "            cat: {feat: idx for idx, feat in enumerate(sorted(features))}\n",
    "            for cat, features in unique_features.items()\n",
    "        }\n",
    "        \n",
    "        self.precomputed_bert_features = {}\n",
    "        self.precomputed_clip_features = {}\n",
    "        \n",
    "        if self.use_txt_features == \"embedded\":\n",
    "            self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "            self.bert_model = BertModel.from_pretrained('bert-base-uncased')\n",
    "            self.bert_model.eval()\n",
    "            self.bert_model.to(self.device)\n",
    "            self.clip_processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "            self.clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "            self.clip_model.eval()\n",
    "            self.clip_model.to(self.device)\n",
    "            self.precompute_features()\n",
    "\n",
    "    def precompute_features(self):\n",
    "        print(\"Precomputing BERT and CLIP features...\")\n",
    "        for img_name in tqdm(self.img_files):\n",
    "            # Precompute BERT features\n",
    "            feature_text = \" \".join(self.features[img_name])\n",
    "            inputs = self.tokenizer(feature_text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "            with torch.no_grad():              \n",
    "                outputs = self.bert_model(**inputs.to(self.device))\n",
    "            bert_features = outputs.last_hidden_state[:, 0, :].squeeze(0)  # Shape: (768,)\n",
    "            self.precomputed_bert_features[img_name] = bert_features.cpu()\n",
    "\n",
    "            # Precompute CLIP features\n",
    "            image_path = os.path.join(self.IMGS_PATH, 'input', img_name)\n",
    "            image = read_image(image_path)\n",
    "            clip_inputs = self.clip_processor(images=image, return_tensors=\"pt\")\n",
    "            with torch.no_grad():           \n",
    "                clip_features = self.clip_model.get_image_features(**clip_inputs.to(self.device))\n",
    "\n",
    "            self.precomputed_clip_features[img_name] = clip_features.squeeze(0).cpu()  # Shape: (512,)\n",
    "        del self.bert_model\n",
    "        del self.tokenizer    \n",
    "        del self.clip_model\n",
    "        del self.clip_processor  \n",
    "        print(\"BERT and CLIP features precomputed and stored.\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_files)\n",
    "\n",
    "    def _locate_test_images(self):\n",
    "        locations = {}\n",
    "        for img in self.img_files:\n",
    "            if os.path.exists(os.path.join(self.IMGS_PATH, 'train', 'input', img)):\n",
    "                locations[img] = 'train'\n",
    "            elif os.path.exists(os.path.join(self.IMGS_PATH, 'test', 'input', img)):\n",
    "                locations[img] = 'test'\n",
    "            else:\n",
    "                raise FileNotFoundError(f\"Image {img} not found in train or test folders\")\n",
    "        return locations\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = self.img_files[idx]\n",
    "        if self.mode == \"test\":\n",
    "            folder = self.img_locations[img_name]\n",
    "            source = read_image(os.path.join(self.IMGS_PATH, folder, 'input', img_name))\n",
    "            target = read_image(os.path.join(self.IMGS_PATH, folder, 'target', img_name))\n",
    "        else:\n",
    "            source = read_image(os.path.join(self.IMGS_PATH, self.mode, 'input', img_name))\n",
    "            target = read_image(os.path.join(self.IMGS_PATH, self.mode, 'target', img_name))\n",
    "\n",
    "        if self.resize:\n",
    "                    source = F.resize(source, (self.image_size, self.image_size), interpolation=F.InterpolationMode.BICUBIC)\n",
    "                    target = F.resize(target, (self.image_size, self.image_size), interpolation=F.InterpolationMode.BICUBIC)\n",
    "\n",
    "        if self.augment_data:\n",
    "            if random.random() > 0.5:\n",
    "                source = F.hflip(source)\n",
    "                target = F.hflip(target)\n",
    "            if random.random() > 0.5:\n",
    "                source = F.vflip(source)\n",
    "                target = F.vflip(target)\n",
    "\n",
    "        if not self.use_txt_features:\n",
    "            return source, target\n",
    "\n",
    "        elif self.use_txt_features == \"one_hot\":\n",
    "            one_hot_features = self.mlb.transform([self.features[img_name]])[0]\n",
    "            one_hot_features = torch.tensor(one_hot_features, dtype=torch.float32)\n",
    "            return source, one_hot_features, target\n",
    "\n",
    "        elif self.use_txt_features == \"categorical\":\n",
    "            cat_features = [self.feature_to_idx[cat][feat] for cat, feat in zip(self.feature_categories, self.features[img_name])]\n",
    "            cat_features = torch.tensor(cat_features, dtype=torch.long)\n",
    "            return source, cat_features, target\n",
    "\n",
    "        elif self.use_txt_features == \"embedded\":\n",
    "            bert_features = self.precomputed_bert_features[img_name]\n",
    "            clip_features = self.precomputed_clip_features[img_name]\n",
    "            return source, bert_features, clip_features, target\n",
    "\n",
    "        else:\n",
    "            raise ValueError(\"Invalid value for use_txt_features. Must be False, 'one_hot', 'categorical', or 'embedded'.\")\n",
    "\n",
    "    def collate_fn(self, batch):\n",
    "        if self.use_txt_features == \"embedded\":\n",
    "            sources, bert_features, clip_features, targets = zip(*batch)\n",
    "            sources = torch.stack(sources)\n",
    "            bert_features = torch.stack(bert_features)\n",
    "            clip_features = torch.stack(clip_features)\n",
    "            targets = torch.stack(targets)\n",
    "            return sources, bert_features, clip_features, targets\n",
    "        else:\n",
    "            sources, features, targets = zip(*batch)\n",
    "            sources = torch.stack(sources)\n",
    "            targets = torch.stack(targets)\n",
    "            if self.use_txt_features in [\"one_hot\", \"categorical\"]:\n",
    "                features = torch.stack(features)\n",
    "            else:\n",
    "                features = None\n",
    "            return sources, features, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset_512 = FiveKDataset(\n",
    "    image_size=256,\n",
    "    mode=\"test\",\n",
    "    resize=False,\n",
    "    augment_data=False,\n",
    "    use_txt_features=False,\n",
    "    device='cuda',\n",
    "    test_file=\"test.txt\"\n",
    ")\n",
    "test_dataset_64 = FiveKDataset(\n",
    "    image_size=64,\n",
    "    mode=\"test\",\n",
    "    resize=True,\n",
    "    augment_data=False,\n",
    "    use_txt_features=False,\n",
    "    device='cuda',\n",
    "    test_file=\"test.txt\"\n",
    ")\n",
    "test_512 = DataLoader(test_dataset_512, batch_size=1, shuffle = False)\n",
    "test_64 = DataLoader(test_dataset_64, batch_size=500 , shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/zakaria/workspace/ai-photo-enhancer/notebooks/../dataset/FiveK'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset_64.IMGS_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "os.chdir('../src')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as transforms\n",
    "from torchvision.transforms import v2\n",
    "from torchmetrics.image import StructuralSimilarityIndexMeasure\n",
    "from envs.new_edit_photo import PhotoEditor\n",
    "from sac.sac_inference import InferenceAgent\n",
    "import yaml\n",
    "from envs.photo_env import PhotoEnhancementEnvTest\n",
    "import numpy as np\n",
    "import argparse\n",
    "import logging\n",
    "import os\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = 'cuda:0'\n",
    "class Config(object):\n",
    "    def __init__(self, dictionary):\n",
    "        self.__dict__.update(dictionary)\n",
    "\n",
    "with open(os.path.join(\"configs/inference_config.yaml\")) as f:\n",
    "    inf_config_dict =yaml.load(f, Loader=yaml.FullLoader)\n",
    "inference_config = Config(inf_config_dict)\n",
    "inference_config.device = DEVICE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/zakaria/workspace/ai-photo-enhancer/src'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "os.chdir('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiments_path = 'experiments/runs/'\n",
    "models_path = [os.path.join(experiments_path,model_path) for model_path in os.listdir(experiments_path)]\n",
    "models_path.remove('experiments/runs/ResNetEncoder__1__1720274282')\n",
    "models_path.remove('experiments/runs/ResNetEncoder__1__1719841165')\n",
    "models_path = models_path[:-1]\n",
    "models_names = [name.split('/')[-1] for name in models_path]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_names = [models_names[-2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ResNetEncoder__solos__2024-07-08_14-39-07',\n",
       " 'ResNetEncoder__fivesliders__2024-07-10_15-57-44',\n",
       " 'ResNetEncoder__fiveslidersaug__2024-07-12_14-59-37',\n",
       " 'ResNetEncoder__sixsliders__2024-07-11_21-02-42',\n",
       " 'SemanticBackbone__semantic_5sliders__2024-07-19_14-08-51',\n",
       " 'ResNetEncoder__allsliders__2024-07-09_16-04-37']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 3, got 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 32\u001b[0m\n\u001b[1;32m     29\u001b[0m inference_env\u001b[38;5;241m.\u001b[39miter_dataloader \u001b[38;5;241m=\u001b[39m \u001b[38;5;28miter\u001b[39m(test_64)\n\u001b[1;32m     30\u001b[0m inference_env\u001b[38;5;241m.\u001b[39mbatch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m500\u001b[39m \n\u001b[0;32m---> 32\u001b[0m batch_64_images \u001b[38;5;241m=\u001b[39m \u001b[43minference_env\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreset\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     33\u001b[0m inf_agent \u001b[38;5;241m=\u001b[39mInferenceAgent(inference_env, inference_config)\n\u001b[1;32m     34\u001b[0m inf_agent\u001b[38;5;241m.\u001b[39mload_backbone(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(model_path,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodels\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbackbone.pth\u001b[39m\u001b[38;5;124m'\u001b[39m))\n",
      "File \u001b[0;32m~/workspace/ai-photo-enhancer/src/envs/photo_env.py:159\u001b[0m, in \u001b[0;36mPhotoEnhancementEnv.reset\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    150\u001b[0m     batch_observation\u001b[38;5;241m=\u001b[39m TensorDict(\n\u001b[1;32m    151\u001b[0m                 {\n\u001b[1;32m    152\u001b[0m                     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch_images\u001b[39m\u001b[38;5;124m\"\u001b[39m:\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msource_image\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    156\u001b[0m                 batch_size \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msource_image\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]],\n\u001b[1;32m    157\u001b[0m             )\n\u001b[1;32m    158\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_txt_features\u001b[38;5;241m==\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mone_hot\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 159\u001b[0m     source_image, txt_features, target_image\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miter_dataloader) \n\u001b[1;32m    160\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    161\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msource_image\u001b[39m\u001b[38;5;124m'\u001b[39m:source_image\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m255.0\u001b[39m,  \n\u001b[1;32m    162\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124menhanced_image\u001b[39m\u001b[38;5;124m'\u001b[39m:source_image\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m255.0\u001b[39m,                     \n\u001b[1;32m    163\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mts_features\u001b[39m\u001b[38;5;124m'\u001b[39m:txt_features,\n\u001b[1;32m    164\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtarget_image\u001b[39m\u001b[38;5;124m'\u001b[39m:target_image\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m255.0\u001b[39m,\n\u001b[1;32m    165\u001b[0m     }\n\u001b[1;32m    166\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miter_dataloader_count \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 3, got 2)"
     ]
    }
   ],
   "source": [
    "all_psnrs = []\n",
    "all_ssims = []\n",
    "for model_path in [models_path[-2]]:\n",
    "    with open(os.path.join(model_path,\"configs/sac_config.yaml\")) as f:\n",
    "        sac_config_dict =yaml.load(f, Loader=yaml.FullLoader)\n",
    "    with open(os.path.join(model_path,\"configs/env_config.yaml\")) as f:\n",
    "        env_config_dict =yaml.load(f, Loader=yaml.FullLoader)\n",
    "\n",
    "    sac_config = Config(sac_config_dict)\n",
    "    env_config = Config(env_config_dict)\n",
    "    sac_config.device = DEVICE\n",
    "\n",
    "    photo_editor = PhotoEditor(env_config.sliders_to_use)\n",
    "\n",
    "    inference_env = PhotoEnhancementEnvTest(\n",
    "                        batch_size=env_config.train_batch_size,\n",
    "                        imsize=env_config.imsize,\n",
    "                        training_mode=False,\n",
    "                        done_threshold=env_config.threshold_psnr,\n",
    "                        edit_sliders=env_config.sliders_to_use,\n",
    "                        features_size=env_config.features_size,\n",
    "                        discretize=env_config.discretize,\n",
    "                        discretize_step= env_config.discretize_step,\n",
    "                        use_txt_features=env_config.use_txt_features if hasattr(env_config,\"use_txt_features\") else False,\n",
    "                        augment_data=False,\n",
    "                        pre_encoding_device=env_config.pre_encoding_device if hasattr(env_config,\"pre_encoding_device\") else 'cpu', \n",
    "                        logger=None)\n",
    "    inference_env.dataloader = test_64\n",
    "    inference_env.iter_dataloader = iter(test_64)\n",
    "    inference_env.batch_size = 500 \n",
    "\n",
    "    batch_64_images = inference_env.reset()\n",
    "    inf_agent =InferenceAgent(inference_env, inference_config)\n",
    "    inf_agent.load_backbone(os.path.join(model_path,'models','backbone.pth'))\n",
    "    inf_agent.load_actor_weights(os.path.join(model_path,'models','actor_head.pth'))\n",
    "    inf_agent.load_critics_weights( os.path.join(model_path,'models','qf1_head.pth'), os.path.join(model_path,'models','qf2_head.pth'))\n",
    "\n",
    "    ssim_metric = StructuralSimilarityIndexMeasure()\n",
    "\n",
    "    \n",
    "    PSNRS = []\n",
    "    SSIM = []\n",
    "\n",
    "    parameters = inf_agent.act(obs=batch_64_images,deterministic=False)\n",
    "\n",
    "    parameter_counter = 0\n",
    "    for i,t in tqdm(test_512, position=0, leave=True):\n",
    "        source = i/255.0\n",
    "        target = t/255.0 \n",
    "        enhanced_image = photo_editor((source.permute(0,2,3,1)).cpu(),parameters[parameter_counter].unsqueeze(0).cpu())\n",
    "        psnr = inference_env.compute_rewards(enhanced_image.permute(0,3,1,2),target).item()+50\n",
    "        ssim = ssim_metric(enhanced_image.permute(0,3,1,2),target).item()\n",
    "        PSNRS.append(psnr)\n",
    "        SSIM.append(ssim)\n",
    "        parameter_counter+=1\n",
    "    mean_PSNRS = round(np.mean(PSNRS),2)\n",
    "    mean_SSIM = round(np.mean(SSIM),3)\n",
    "    all_psnrs.append(mean_PSNRS)\n",
    "    all_ssims.append(mean_SSIM)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "photoen",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
