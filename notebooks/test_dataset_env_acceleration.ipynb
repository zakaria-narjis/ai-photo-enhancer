{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision.io import read_image\n",
    "import torchvision.transforms.v2.functional as F\n",
    "import os\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from transformers import BertTokenizer, BertModel, CLIPProcessor, CLIPModel\n",
    "from tqdm import tqdm\n",
    "\n",
    "class FiveKDataset(Dataset):\n",
    "    def __init__(self, image_size, mode=\"train\", resize=True, \n",
    "                 augment_data=False, use_txt_features=False, device='cuda'):\n",
    "        current_dir = os.getcwd()\n",
    "        dataset_dir = os.path.join(current_dir, \"..\",\"dataset\")\n",
    "        self.IMGS_PATH = os.path.join(dataset_dir, f\"FiveK/{mode}\")\n",
    "        self.FEATURES_PATH = os.path.join(dataset_dir, \"processed_categories_2.txt\")\n",
    "        \n",
    "        self.resize = resize\n",
    "        self.image_size = image_size\n",
    "        self.augment_data = augment_data\n",
    "        self.use_txt_features = use_txt_features\n",
    "        self.device = device\n",
    "        self.feature_categories = [\"Location\", \"Time\", \"Light\", \"Subject\"]\n",
    "        \n",
    "        # Load semantic features from processed_categories.txt\n",
    "        self.features = {}\n",
    "        with open(self.FEATURES_PATH, 'r') as f:\n",
    "            for line in f:\n",
    "                parts = line.strip().split(',')\n",
    "                img_name = parts[0]\n",
    "                img_features = parts[1:]\n",
    "                self.features[img_name] = img_features\n",
    "        \n",
    "        # Load image files\n",
    "        self.img_files = [f for f in os.listdir(os.path.join(self.IMGS_PATH, 'input'))]\n",
    "        \n",
    "        # Prepare MultiLabelBinarizer\n",
    "        all_features = [self.features[img] for img in self.img_files]\n",
    "        self.mlb = MultiLabelBinarizer()\n",
    "        self.mlb.fit(all_features)\n",
    "        \n",
    "        # Create encoding dictionaries for categorical approach\n",
    "        unique_features = {cat: set(feat[i] for feat in all_features) \n",
    "                           for i, cat in enumerate(self.feature_categories)}\n",
    "        self.feature_to_idx = {\n",
    "            cat: {feat: idx for idx, feat in enumerate(sorted(features))}\n",
    "            for cat, features in unique_features.items()\n",
    "        }\n",
    "        \n",
    "        # Preload all images and features\n",
    "        self.preload_data()\n",
    "        \n",
    "        if self.use_txt_features == \"embedded\":\n",
    "            self.precompute_features()\n",
    "\n",
    "    def preload_data(self):\n",
    "        print(\"Preloading images and features...\")\n",
    "        self.source_images = []\n",
    "        self.target_images = []\n",
    "        self.one_hot_features = []\n",
    "        self.cat_features = []\n",
    "        \n",
    "        for img_name in tqdm(self.img_files):\n",
    "            # Load and preprocess images\n",
    "            source = read_image(os.path.join(self.IMGS_PATH, 'input', img_name))\n",
    "            target = read_image(os.path.join(self.IMGS_PATH, 'target', img_name))\n",
    "            \n",
    "            if self.resize:\n",
    "                source = F.resize(source, (self.image_size, self.image_size), interpolation=F.InterpolationMode.BICUBIC)\n",
    "                target = F.resize(target, (self.image_size, self.image_size), interpolation=F.InterpolationMode.BICUBIC)\n",
    "            \n",
    "            self.source_images.append(source.to(self.device))\n",
    "            self.target_images.append(target.to(self.device))\n",
    "            \n",
    "            # Precompute features\n",
    "            if self.use_txt_features == \"one_hot\":\n",
    "                one_hot = self.mlb.transform([self.features[img_name]])[0]\n",
    "                self.one_hot_features.append(torch.tensor(one_hot, dtype=torch.float32, device=self.device))\n",
    "            elif self.use_txt_features == \"categorical\":\n",
    "                cat = [self.feature_to_idx[cat][feat] for cat, feat in zip(self.feature_categories, self.features[img_name])]\n",
    "                self.cat_features.append(torch.tensor(cat, dtype=torch.long, device=self.device))\n",
    "        \n",
    "        self.source_images = torch.stack(self.source_images)\n",
    "        self.target_images = torch.stack(self.target_images)\n",
    "        \n",
    "        if self.use_txt_features == \"one_hot\":\n",
    "            self.one_hot_features = torch.stack(self.one_hot_features)\n",
    "        elif self.use_txt_features == \"categorical\":\n",
    "            self.cat_features = torch.stack(self.cat_features)\n",
    "        \n",
    "        print(\"Images and features preloaded and stored in GPU memory.\")\n",
    "\n",
    "    def precompute_features(self):\n",
    "        print(\"Precomputing BERT and CLIP features...\")\n",
    "        self.bert_features = []\n",
    "        self.clip_features = []\n",
    "        \n",
    "        tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "        bert_model = BertModel.from_pretrained('bert-base-uncased').to(self.device).eval()\n",
    "        clip_processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "        clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\").to(self.device).eval()\n",
    "        \n",
    "        for img_name in tqdm(self.img_files):\n",
    "            # Precompute BERT features\n",
    "            feature_text = \" \".join(self.features[img_name])\n",
    "            inputs = tokenizer(feature_text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512).to(self.device)\n",
    "            with torch.no_grad():              \n",
    "                outputs = bert_model(**inputs)\n",
    "            bert_features = outputs.last_hidden_state[:, 0, :].squeeze(0)  # Shape: (768,)\n",
    "            self.bert_features.append(bert_features)\n",
    "\n",
    "            # Precompute CLIP features\n",
    "            image = self.source_images[len(self.bert_features) - 1].cpu()  # Get the corresponding preloaded image\n",
    "            clip_inputs = clip_processor(images=image, return_tensors=\"pt\").to(self.device)\n",
    "            with torch.no_grad():           \n",
    "                clip_features = clip_model.get_image_features(**clip_inputs)\n",
    "            self.clip_features.append(clip_features.squeeze(0))  # Shape: (512,)\n",
    "        \n",
    "        self.bert_features = torch.stack(self.bert_features)\n",
    "        self.clip_features = torch.stack(self.clip_features)\n",
    "        \n",
    "        del bert_model, tokenizer, clip_model, clip_processor\n",
    "        print(\"BERT and CLIP features precomputed and stored in GPU memory.\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        source = self.source_images[idx]\n",
    "        target = self.target_images[idx]\n",
    "\n",
    "        if self.augment_data:\n",
    "            if torch.rand(1).item() > 0.5:\n",
    "                source = F.hflip(source)\n",
    "                target = F.hflip(target)\n",
    "            if torch.rand(1).item() > 0.5:\n",
    "                source = F.vflip(source)\n",
    "                target = F.vflip(target)\n",
    "\n",
    "        if not self.use_txt_features:\n",
    "            return source, target\n",
    "        elif self.use_txt_features == \"one_hot\":\n",
    "            return source, self.one_hot_features[idx], target\n",
    "        elif self.use_txt_features == \"categorical\":\n",
    "            return source, self.cat_features[idx], target\n",
    "        elif self.use_txt_features == \"embedded\":\n",
    "            return source, self.bert_features[idx], self.clip_features[idx], target\n",
    "        else:\n",
    "            raise ValueError(\"Invalid value for use_txt_features. Must be False, 'one_hot', 'categorical', or 'embedded'.\")\n",
    "\n",
    "    def collate_fn(self, batch):\n",
    "        if self.use_txt_features == \"embedded\":\n",
    "            sources, bert_features, clip_features, targets = zip(*batch)\n",
    "            return torch.stack(sources), torch.stack(bert_features), torch.stack(clip_features), torch.stack(targets)\n",
    "        else:\n",
    "            sources, features, targets = zip(*batch)\n",
    "            return torch.stack(sources), torch.stack(features) if features[0] is not None else None, torch.stack(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f=FiveKDataset(64, mode=\"train\", use_txt_features=False, device='cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.envs.new_edit_photo import PhotoEditor\n",
    "parameters = torch.rand(500,15)\n",
    "fake_images = torch.rand(500,64,64,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = parameters.cuda()\n",
    "fake_images = fake_images.cuda()\n",
    "editor = PhotoEditor([\"contrast\",\"exposure\",\"shadows\",\"highlights\",\"whites\",\"blacks\",\"temp\",\"vibrance\",\"saturation\",\"tint\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "editor(fake_images, parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import models\n",
    "from torchvision import transforms\n",
    "import torch.nn.init as init\n",
    "LOG_STD_MAX = 3\n",
    "LOG_STD_MIN = -5\n",
    "\n",
    "\n",
    "class ResNETBackbone(nn.Module):\n",
    "    def __init__(self,):\n",
    "        super().__init__()\n",
    "        self.model = models.resnet18(weights='ResNet18_Weights.DEFAULT')\n",
    "        self.model = torch.nn.Sequential(*(list(self.model.children())[:-1]))\n",
    "        self.preprocess = transforms.Compose([\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])#remove classifier\n",
    "    def forward(self,batch_images):\n",
    "        x = self.preprocess (batch_images)\n",
    "        features = self.model(x)\n",
    "        features=torch.flatten(features,start_dim=-3,end_dim=-1)\n",
    "        return features\n",
    "a=ResNETBackbone()       \n",
    "a.model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision.io import read_image\n",
    "import torchvision.transforms.v2.functional as F\n",
    "import os\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from transformers import BertTokenizer, BertModel, CLIPProcessor, CLIPModel\n",
    "from tqdm import tqdm\n",
    "\n",
    "class FiveKDataset(Dataset):\n",
    "    def __init__(self, image_size, mode=\"train\", resize=True, \n",
    "                 augment_data=False, use_txt_features=False, device='cuda',\n",
    "                 pre_load_images=True):\n",
    "        current_dir = os.getcwd()\n",
    "        dataset_dir = os.path.join(current_dir, \"dataset\")\n",
    "        self.IMGS_PATH = os.path.join(dataset_dir, f\"FiveK/{mode}\")\n",
    "        self.FEATURES_PATH = os.path.join(dataset_dir, \"processed_categories_2.txt\")\n",
    "        \n",
    "        self.resize = resize\n",
    "        self.image_size = image_size\n",
    "        self.augment_data = augment_data\n",
    "        self.use_txt_features = use_txt_features\n",
    "        self.device = device\n",
    "        self.pre_load_images = pre_load_images\n",
    "        self.feature_categories = [\"Location\", \"Time\", \"Light\", \"Subject\"]\n",
    "        \n",
    "        # Load semantic features from processed_categories.txt\n",
    "        self.features = {}\n",
    "        with open(self.FEATURES_PATH, 'r') as f:\n",
    "            for line in f:\n",
    "                parts = line.strip().split(',')\n",
    "                img_name = parts[0]\n",
    "                img_features = parts[1:]\n",
    "                self.features[img_name] = img_features\n",
    "        \n",
    "        # Load image files\n",
    "        self.img_files = [f for f in os.listdir(os.path.join(self.IMGS_PATH, 'input'))]\n",
    "        \n",
    "        # Prepare MultiLabelBinarizer\n",
    "        all_features = [self.features[img] for img in self.img_files]\n",
    "        self.mlb = MultiLabelBinarizer()\n",
    "        self.mlb.fit(all_features)\n",
    "        \n",
    "        # Create encoding dictionaries for categorical approach\n",
    "        unique_features = {cat: set(feat[i] for feat in all_features) \n",
    "                           for i, cat in enumerate(self.feature_categories)}\n",
    "        self.feature_to_idx = {\n",
    "            cat: {feat: idx for idx, feat in enumerate(sorted(features))}\n",
    "            for cat, features in unique_features.items()\n",
    "        }\n",
    "        \n",
    "        # Preload all images and features if pre_load_images is True\n",
    "        if self.pre_load_images:\n",
    "            self.preload_data()\n",
    "        \n",
    "        if self.use_txt_features == \"embedded\":\n",
    "            self.precompute_features()\n",
    "\n",
    "    def preload_data(self):\n",
    "        print(\"Preloading images and features...\")\n",
    "        self.source_images = []\n",
    "        self.target_images = []\n",
    "        self.one_hot_features = []\n",
    "        self.cat_features = []\n",
    "        \n",
    "        for img_name in tqdm(self.img_files):\n",
    "            # Load and preprocess images\n",
    "            source = read_image(os.path.join(self.IMGS_PATH, 'input', img_name))\n",
    "            target = read_image(os.path.join(self.IMGS_PATH, 'target', img_name))\n",
    "            \n",
    "            if self.resize:\n",
    "                source = F.resize(source, (self.image_size, self.image_size), interpolation=F.InterpolationMode.BICUBIC)\n",
    "                target = F.resize(target, (self.image_size, self.image_size), interpolation=F.InterpolationMode.BICUBIC)\n",
    "            \n",
    "            self.source_images.append(source.to(self.device))\n",
    "            self.target_images.append(target.to(self.device))\n",
    "            \n",
    "            # Precompute features\n",
    "            if self.use_txt_features == \"one_hot\":\n",
    "                one_hot = self.mlb.transform([self.features[img_name]])[0]\n",
    "                self.one_hot_features.append(torch.tensor(one_hot, dtype=torch.float32, device=self.device))\n",
    "            elif self.use_txt_features == \"categorical\":\n",
    "                cat = [self.feature_to_idx[cat][feat] for cat, feat in zip(self.feature_categories, self.features[img_name])]\n",
    "                self.cat_features.append(torch.tensor(cat, dtype=torch.long, device=self.device))\n",
    "        \n",
    "        self.source_images = torch.stack(self.source_images)\n",
    "        self.target_images = torch.stack(self.target_images)\n",
    "        \n",
    "        if self.use_txt_features == \"one_hot\":\n",
    "            self.one_hot_features = torch.stack(self.one_hot_features)\n",
    "        elif self.use_txt_features == \"categorical\":\n",
    "            self.cat_features = torch.stack(self.cat_features)\n",
    "        \n",
    "        print(\"Images and features preloaded and stored in GPU memory.\")\n",
    "\n",
    "    def precompute_features(self):\n",
    "        print(\"Precomputing BERT and CLIP features...\")\n",
    "        self.bert_features = []\n",
    "        self.clip_features = []\n",
    "        \n",
    "        tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "        bert_model = BertModel.from_pretrained('bert-base-uncased').to(self.device).eval()\n",
    "        clip_processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "        clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\").to(self.device).eval()\n",
    "        \n",
    "        for img_name in tqdm(self.img_files):\n",
    "            # Precompute BERT features\n",
    "            feature_text = \" \".join(self.features[img_name])\n",
    "            inputs = tokenizer(feature_text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512).to(self.device)\n",
    "            with torch.no_grad():              \n",
    "                outputs = bert_model(**inputs)\n",
    "            bert_features = outputs.last_hidden_state[:, 0, :].squeeze(0)  # Shape: (768,)\n",
    "            self.bert_features.append(bert_features)\n",
    "\n",
    "            # Precompute CLIP features\n",
    "            image = self.source_images[len(self.bert_features) - 1].cpu()  # Get the corresponding preloaded image\n",
    "            clip_inputs = clip_processor(images=image, return_tensors=\"pt\").to(self.device)\n",
    "            with torch.no_grad():           \n",
    "                clip_features = clip_model.get_image_features(**clip_inputs)\n",
    "            self.clip_features.append(clip_features.squeeze(0))  # Shape: (512,)\n",
    "        \n",
    "        self.bert_features = torch.stack(self.bert_features).to(self.device)\n",
    "        self.clip_features = torch.stack(self.clip_features).to(self.device)\n",
    "        \n",
    "        del bert_model, tokenizer, clip_model, clip_processor\n",
    "        print(\"BERT and CLIP features precomputed and stored in GPU memory.\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if self.pre_load_images:\n",
    "            source = self.source_images[idx]\n",
    "            target = self.target_images[idx]\n",
    "        else:\n",
    "            img_name = self.img_files[idx]\n",
    "            source = read_image(os.path.join(self.IMGS_PATH, 'input', img_name))\n",
    "            target = read_image(os.path.join(self.IMGS_PATH, 'target', img_name))\n",
    "            \n",
    "            if self.resize:\n",
    "                source = F.resize(source, (self.image_size, self.image_size), interpolation=F.InterpolationMode.BICUBIC)\n",
    "                target = F.resize(target, (self.image_size, self.image_size), interpolation=F.InterpolationMode.BICUBIC)\n",
    "            \n",
    "            source = source.to(self.device)\n",
    "            target = target.to(self.device)\n",
    "\n",
    "        if self.augment_data:\n",
    "            if torch.rand(1).item() > 0.5:\n",
    "                source = F.hflip(source)\n",
    "                target = F.hflip(target)\n",
    "            if torch.rand(1).item() > 0.5:\n",
    "                source = F.vflip(source)\n",
    "                target = F.vflip(target)\n",
    "\n",
    "        if not self.use_txt_features:\n",
    "            return source, target\n",
    "        elif self.use_txt_features == \"one_hot\":\n",
    "            if self.pre_load_images:\n",
    "                return source, self.one_hot_features[idx], target\n",
    "            else:\n",
    "                one_hot = self.mlb.transform([self.features[self.img_files[idx]]])[0]\n",
    "                return source, torch.tensor(one_hot, dtype=torch.float32, device=self.device), target\n",
    "        elif self.use_txt_features == \"categorical\":\n",
    "            if self.pre_load_images:\n",
    "                return source, self.cat_features[idx], target\n",
    "            else:\n",
    "                cat = [self.feature_to_idx[cat][feat] for cat, feat in zip(self.feature_categories, self.features[self.img_files[idx]])]\n",
    "                return source, torch.tensor(cat, dtype=torch.long, device=self.device), target\n",
    "        elif self.use_txt_features == \"embedded\":\n",
    "            return source, self.bert_features[idx], self.clip_features[idx], target\n",
    "        else:\n",
    "            raise ValueError(\"Invalid value for use_txt_features. Must be False, 'one_hot', 'categorical', or 'embedded'.\")\n",
    "\n",
    "    def collate_fn(self, batch):\n",
    "        if self.use_txt_features == \"embedded\":\n",
    "            sources, bert_features, clip_features, targets = zip(*batch)\n",
    "            return torch.stack(sources), torch.stack(bert_features), torch.stack(clip_features), torch.stack(targets)\n",
    "        else:\n",
    "            sources, features, targets = zip(*batch)\n",
    "            return torch.stack(sources), torch.stack(features) if features[0] is not None else None, torch.stack(targets)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "photoen",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
