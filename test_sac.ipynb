{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import time\n",
    "from envs.photo_env import PhotoEnhancementEnv\n",
    "from envs.photo_env import PhotoEnhancementEnvTest\n",
    "from sac.sac_algorithm import SAC\n",
    "import multiprocessing as mp\n",
    "try:\n",
    "    mp.set_start_method('spawn', force=True)\n",
    "except RuntimeError:\n",
    "    pass  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = PhotoEnhancementEnv()\n",
    "test_env = PhotoEnhancementEnvTest()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"configs/hyperparameters.yaml\") as f:\n",
    "    config_dict =yaml.load(f, Loader=yaml.FullLoader)\n",
    "    \n",
    "class Config(object):\n",
    "    def __init__(self, dictionary):\n",
    "        self.__dict__.update(dictionary)\n",
    "sac_config = Config(config_dict)\n",
    "\n",
    "SEED = sac_config.seed\n",
    "DEVICE= 'CUDA'\n",
    "\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = sac_config.torch_deterministic\n",
    "torch.autograd.set_detect_anomaly(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_name = f\"{sac_config.exp_name}__{sac_config.seed}__{int(time.time())}\"\n",
    "writer = SummaryWriter(f\"runs/{run_name}\")\n",
    "writer.add_text(\n",
    "    \"hyperparameters\",\n",
    "    \"|param|value|\\n|-|-|\\n%s\" % (\"\\n\".join([f\"|{key}|{value}|\" for key, value in vars(sac_config).items()])),\n",
    ")\n",
    "agent = SAC(env,sac_config,writer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False])\n",
      "tensor([])\n",
      "tensor([-0.0855, -0.2336, -0.1189, -0.1057, -0.0802, -0.1202, -0.0797, -0.2552,\n",
      "        -0.0652, -0.1022, -0.0857, -0.0501, -0.2029, -0.2656, -0.1410, -0.0995,\n",
      "        -0.0713, -0.1752, -0.0602, -0.0384, -0.3845, -0.2845, -0.2289, -0.0764,\n",
      "        -0.2044, -0.0637, -0.0520, -0.1045, -0.0801, -0.1407, -0.1775, -0.1949])\n",
      "tensor([False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False])\n",
      "tensor([])\n",
      "tensor([-0.1070, -0.2030, -0.1092, -0.1119, -0.1349, -0.1120, -0.1036, -0.0789,\n",
      "        -0.0921, -0.0836, -0.0633, -0.1977, -0.1084, -0.0573, -0.0834, -0.0925,\n",
      "        -0.2877, -0.0857, -0.0212, -0.0776, -0.0674, -0.2647, -0.1576, -0.1875,\n",
      "        -0.1786, -0.1126, -0.0732, -0.1478, -0.1214, -0.1177, -0.0542, -0.1569])\n",
      "tensor([False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False])\n",
      "tensor([])\n",
      "tensor([-0.1078, -0.1320, -0.1688, -0.0453, -0.0880, -0.1549, -0.0707, -0.1072,\n",
      "        -0.1660, -0.2140, -0.1388, -0.1621, -0.0844, -0.0506, -0.1807, -0.1779,\n",
      "        -0.0567, -0.0849, -0.2491, -0.0768, -0.0950, -0.2251, -0.1570, -0.1040,\n",
      "        -0.1688, -0.1000, -0.1222, -0.1848, -0.0674, -0.1775, -0.1648, -0.0902])\n",
      "tensor([False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False])\n",
      "tensor([])\n",
      "tensor([-0.0579, -0.0511, -0.0763, -0.1213, -0.1495, -0.0898, -0.2555, -0.1961,\n",
      "        -0.0668, -0.0703, -0.1144, -0.0488, -0.1538, -0.1754, -0.1613, -0.0889,\n",
      "        -0.1321, -0.1700, -0.1041, -0.1699, -0.2084, -0.1345, -0.1170, -0.1548,\n",
      "        -0.1194, -0.1686, -0.1489, -0.2612, -0.1139, -0.1787, -0.1079, -0.0395])\n",
      "tensor([False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False])\n",
      "tensor([])\n",
      "tensor([-0.0877, -0.1966, -0.0580, -0.1553, -0.0808, -0.0521, -0.1191, -0.1338,\n",
      "        -0.1039, -0.0923, -0.1439, -0.0828, -0.0880, -0.1207, -0.1172, -0.0515,\n",
      "        -0.1631, -0.0563, -0.0841, -0.1178, -0.1725, -0.0757, -0.1791, -0.3241,\n",
      "        -0.1124, -0.2106, -0.1458, -0.1950, -0.1380, -0.0852, -0.1349, -0.1196])\n",
      "tensor([False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False])\n",
      "tensor([])\n",
      "tensor([-0.1394, -0.2239, -0.0888, -0.1414, -0.0906, -0.1216, -0.1784, -0.1461,\n",
      "        -0.2165, -0.0786, -0.0729, -0.0324, -0.1258, -0.0702, -0.1690, -0.0558,\n",
      "        -0.0584, -0.1126, -0.0683, -0.1794, -0.0899, -0.0734, -0.0759, -0.1233,\n",
      "        -0.0563, -0.0421, -0.1441, -0.2692, -0.0620, -0.2075, -0.1222, -0.1650])\n",
      "tensor([False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False])\n",
      "tensor([])\n",
      "tensor([-0.0493, -0.0804, -0.1067, -0.0597, -0.2132, -0.2301, -0.0825, -0.0358,\n",
      "        -0.0696, -0.0581, -0.0939, -0.0841, -0.0835, -0.0832, -0.1096, -0.0908,\n",
      "        -0.0851, -0.0871, -0.0271, -0.0348, -0.1713, -0.2105, -0.1231, -0.2693,\n",
      "        -0.2747, -0.1471, -0.1707, -0.0881, -0.1215, -0.1133, -0.0406, -0.1056])\n",
      "tensor([False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False])\n",
      "tensor([])\n",
      "tensor([-0.1249, -0.0600, -0.0566, -0.0997, -0.0975, -0.0547, -0.1838, -0.1075,\n",
      "        -0.1510, -0.1188, -0.0675, -0.2714, -0.0839, -0.1538, -0.1594, -0.1343,\n",
      "        -0.0970, -0.1406, -0.0766, -0.2340, -0.1582, -0.0465, -0.1089, -0.1498,\n",
      "        -0.0869, -0.1055, -0.1895, -0.1020, -0.1682, -0.0720, -0.0637, -0.0574])\n"
     ]
    }
   ],
   "source": [
    "agent.start_time = time.time()\n",
    "for i in range(sac_config.total_timesteps):\n",
    "    episode_count = 0\n",
    "    \n",
    "    agent.reset_env()\n",
    "    envs_mean_rewards =[]\n",
    "    while True:     \n",
    "        episode_count+=1\n",
    "        agent.global_step+=1\n",
    "        rewards,batch_dones = agent.train()\n",
    "        envs_mean_rewards.append(rewards.mean().item())\n",
    "        if(batch_dones==True).any():\n",
    "            # print('one done')\n",
    "            # print(agent.state.shape,agent.env.sub_env_running.shape)\n",
    "            num_env_done = int(batch_dones.sum().item())\n",
    "            agent.writer.add_scalar(\"charts/num_env_done\", num_env_done , agent.global_step)\n",
    "        if agent.global_step % 100 == 0:\n",
    "            ens_mean_episodic_return = sum(envs_mean_rewards)\n",
    "            agent.writer.add_scalar(\"charts/mean_episodic_return\", ens_mean_episodic_return, agent.global_step)\n",
    "\n",
    "        if (batch_dones==True).all()==True or episode_count==sac_config.max_episode_timesteps:\n",
    "            episode_count=0           \n",
    "            break \n",
    "\n",
    "    if agent.global_step%200==0:\n",
    "        agent.backbone.eval()\n",
    "        with torch.no_grad():\n",
    "            n_images = 5\n",
    "            obs = test_env.reset() \n",
    "            actions = agent.actor.get_action(obs.to(sac_config.device))\n",
    "            _,rewards,dones = test_env.step(actions[0].cpu())\n",
    "            agent.writer.add_scalar(\"charts/test_mean_episodic_return\", rewards.mean().item(), agent.global_step)\n",
    "            agent.writer.add_images(\"test_images\",test_env.state['source_image'][:n_images],0)\n",
    "            agent.writer.add_images(\"test_images\",test_env.state['enhanced_image'][:n_images],1)\n",
    "            agent.writer.add_images(\"test_images\",test_env.state['target_image'][:n_images],2)\n",
    "        agent.backbone.train()\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def sample_near_values_batch(tensor, batch_size, std_dev=0.05, clip_min=0.0, clip_max=1.0):\n",
    "    \"\"\"\n",
    "    Generate a batch of sampled values near the given tensor.\n",
    "    \n",
    "    Args:\n",
    "    tensor (torch.Tensor): The input tensor to sample near.\n",
    "    batch_size (int): The number of samples to generate.\n",
    "    std_dev (float): Standard deviation for the normal distribution.\n",
    "    clip_min (float): Minimum value to clip the result.\n",
    "    clip_max (float): Maximum value to clip the result.\n",
    "    \n",
    "    Returns:\n",
    "    torch.Tensor: A batch of tensors with sampled values.\n",
    "    \"\"\"\n",
    "    # Expand the input tensor to the desired batch size\n",
    "    batched_tensor = tensor.unsqueeze(0).expand(batch_size, -1)\n",
    "    \n",
    "    # Create a noise tensor with the same shape as the batched tensor\n",
    "    noise = torch.randn_like(batched_tensor) * std_dev\n",
    "    \n",
    "    # Add the noise to the batched tensor\n",
    "    sampled = batched_tensor + noise\n",
    "    \n",
    "    # Clip the values to ensure they're within the specified range\n",
    "    sampled = torch.clamp(sampled, clip_min, clip_max)\n",
    "    \n",
    "    return sampled\n",
    "\n",
    "# Your original tensor\n",
    "original_tensor = torch.tensor([0.125, 0.125, 0.375, 0.125, 0., 0.0625, 0.9375, 0.375, 0.0625, 0., 0.125, 0.125])\n",
    "\n",
    "# Set the desired batch size\n",
    "batch_size = 5\n",
    "\n",
    "# Generate a batch of sampled values\n",
    "sampled_batch = sample_near_values_batch(original_tensor, batch_size)\n",
    "sampled_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "source_image = cv2.imread(\"sample_images/a0001-jmac_DSC1459.jpg\")\n",
    "target_image = cv2.imread(\"sample_images/a0676-kme_609_C.jpg\")\n",
    "source_image = cv2.cvtColor(source_image, cv2.COLOR_BGR2RGB) \n",
    "target_image = cv2.cvtColor(target_image, cv2.COLOR_BGR2RGB) \n",
    "source_image = cv2.resize(source_image, (64, 64)) / 255.0\n",
    "target_image = cv2.resize(target_image, (64, 64)) / 255.0\n",
    "\n",
    "input = torch.Tensor(source_image).permute(2,0,1).unsqueeze(0)\n",
    "\n",
    "enhanced_image = input.clone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(target_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(source_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from envs.features_extractor import ResnetEncoder\n",
    "from envs.new_edit_photo import PhotoEditor\n",
    "import matplotlib.pyplot as plt\n",
    "photo_editor = PhotoEditor()\n",
    "image_encoder = ResnetEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.global_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_source = image_encoder.encode(input)\n",
    "original_64 = input.permute(0,2,3,1)\n",
    "original_image = torch.Tensor(source_image).unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param = torch.tensor([0.125, 0.125, 0.375, 0.125, 0., 0.0625, 0.9375, 0.375, 0.0625, 0., 0.125, 0.125]).unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enhanced_image=enhanced_image.permute(0,2,3,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1):\n",
    "    with torch.no_grad():\n",
    "        encoded_enhanced = image_encoder.encode(enhanced_image.permute(0,3,1,2))\n",
    "        encoded_input = torch.cat([encoded_source,encoded_enhanced],dim=1)\n",
    "        parameters = agent.actor.get_action(encoded_input)\n",
    "        enhanced_image = photo_editor(original_64.cpu(),parameters[0].cpu())\n",
    "enhanced_image_512 = photo_editor(original_image.cpu(),parameters[0].cpu())\n",
    "parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enhanced_image.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enhanced =torch.flatten(enhanced_image_512.clone(),start_dim=1, end_dim=-1)\n",
    "target = torch.flatten(original_image.clone(),start_dim=1, end_dim=-1)\n",
    "\n",
    "rmse = enhanced-target\n",
    "rmse = torch.pow(rmse,2).mean(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(enhanced_image_512[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "photoen",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
